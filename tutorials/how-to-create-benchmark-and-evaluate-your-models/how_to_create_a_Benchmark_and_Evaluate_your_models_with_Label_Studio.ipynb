{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating models is only as good as the benchmark you test them against.  \n",
        "In this tutorial, you'll learn how to use **Label Studio** to create a high-quality benchmark dataset, label it with human expertise, and then evaluate multiple AI models against it ‚Äî all using the **Label Studio SDK**.  \n",
        "\n",
        "By the end, you'll have a reproducible workflow to **measure and compare model performance** on your own data ‚Äî moving beyond intuition and into measurable, data-driven insights.\n"
      ],
      "metadata": {
        "id": "v-NOwX1jcg-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Studio Requirements\n",
        "\n",
        "This tutorial showcases one or more features available only in Label Studio paid products. We recommend [creating a Starter Cloud trial](https://app.humansignal.com/user/cloud-trial?offer=d9a5&) to follow the tutorial."
      ],
      "metadata": {
        "id": "lDDShMwy6dg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† What are benchmarks?\n",
        "\n",
        "A **benchmark** is a trusted, labeled dataset used as a ground truth reference to evaluate model performance.  \n",
        "Think of it as your *reality check* ‚Äî it lets you see how well a model really understands your domain and tasks.  \n",
        "\n",
        "Unlike public leaderboards or off-the-shelf test sets, a **custom benchmark** reflects *your data, your domain, and your definition of success*.  \n",
        "For example, if you're detecting phishing emails, your benchmark should include real examples of legitimate and malicious messages relevant to your context.\n",
        "\n",
        "üëâ To learn more about the role of benchmarks in AI evaluation, see [**Why Benchmarks Matter for Evaluating LLMs**](https://labelstud.io/blog/why-benchmarks-matter-for-evaluating-llms/).\n"
      ],
      "metadata": {
        "id": "KHpOTyV1hWsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öñÔ∏è Why are benchmarks important?\n",
        "\n",
        "Benchmarks are the backbone of **model evaluation and continuous improvement**. Without them, it‚Äôs hard to know if a new model or prompt version actually performs better ‚Äî or just seems to.  \n",
        "\n",
        "A good benchmark lets you:\n",
        "- üîÅ Compare models from different providers on the *same data*  \n",
        "- üß© Identify strengths and weaknesses across labels or categories  \n",
        "- üìà Track progress over time as your models evolve  \n",
        "- ‚úÖ Build trust in your model‚Äôs predictions through measurable accuracy  \n",
        "\n",
        "In other words, benchmarks turn subjective ‚Äúit looks better‚Äù claims into **objective evidence**.  \n",
        "For a deeper look at this idea, check out [**How to Build AI Benchmarks That Evolve with Your Models**](https://labelstud.io/blog/how-to-build-ai-benchmarks-that-evolve-with-your-models/).\n"
      ],
      "metadata": {
        "id": "ByRkXaL6hYSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è How can I use Label Studio to create my benchmark?\n",
        "\n",
        "Label Studio gives you everything you need to create, manage, and evolve your benchmark ‚Äî all in one platform.  \n",
        "With it, you can:\n",
        "- üì• **Import and label your data** collaboratively or programmatically  \n",
        "- üß≠ **Version and refine** your benchmark as your understanding improves  \n",
        "- ü§ñ **Run model evaluations** directly using the SDK and built-in prompt management tools  \n",
        "\n",
        "This makes your benchmarks *living assets* that adapt as your data and models grow.  \n",
        "For inspiration, see how others are building domain-specific benchmarks in [**How LegalBenchmarks.AI Built a Domain-Specific AI Benchmark**](https://labelstud.io/blog/how-legalbenchmarks-ai-built-a-domain-specific-ai-benchmark/).\n"
      ],
      "metadata": {
        "id": "kS3nL2dSh1Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéì What is this tutorial going to teach me?\n",
        "\n",
        "In this tutorial, you‚Äôll learn how to:\n",
        "1. üèóÔ∏è Create a benchmark project in Label Studio using the SDK  \n",
        "2. üìä Import and label your own dataset  \n",
        "3. üí¨ Define prompts and model versions to evaluate  \n",
        "4. üìâ Run evaluations and visualize results  \n",
        "\n",
        "We‚Äôll also walk through an example comparing several models on a phishing email classification task ‚Äî similar to how we evaluated models in [**Evaluating the GPT-5 Series on Custom Benchmarks**](https://labelstud.io/blog/evaluating-the-gpt-5-series-on-custom-benchmarks/).  \n",
        "\n",
        "By the end, you‚Äôll have a repeatable workflow to **evaluate any model** on **any dataset** ‚Äî grounded in your own benchmark."
      ],
      "metadata": {
        "id": "2w5l4Jjzh9tF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Before You Start\n",
        "\n",
        "Before diving into the code, let‚Äôs make sure you have everything set up to follow along.  \n",
        "\n",
        "You‚Äôll only need a few things to get started üëá\n",
        "\n",
        "### üß∞ Prerequisites\n",
        "> **Note:** This tutorial is available for **Label Studio Enterprise** and **Starter Cloud** users, since it uses features like **Prompts** and **Agreement Metrics** that aren‚Äôt available in the open-source version.\n",
        "\n",
        "If you‚Äôre using the open-source version of Label Studio, you can still follow along conceptually.  \n",
        "üëâ Learn more or request access at [**humansignal.com**](https://humansignal.com/).\n",
        "\n",
        "You‚Äôll need:\n",
        "- A **Label Studio account** on [app.humansignal.com](https://app.humansignal.com/)  \n",
        "  _(Starter Cloud or Enterprise edition required)_  \n",
        "- An **API key** from your user settings (you‚Äôll use it to connect via the SDK)  \n",
        "- Access to a **workspace** where you can create projects\n",
        "\n",
        "\n",
        "### üß© What you‚Äôll be using\n",
        "We‚Äôll use the **Label Studio SDK** to:\n",
        "- Connect to your Label Studio instance  \n",
        "- Create a project and import data  \n",
        "- Define prompts and model versions  \n",
        "- Run model evaluations and visualize the results  \n",
        "\n",
        "Everything happens programmatically ‚Äî no need to click around the UI (though you can open your Label Studio anytime to see what‚Äôs happening üëÄ).\n",
        "\n",
        "Let's dive in üöÄ\n"
      ],
      "metadata": {
        "id": "ogggN6ANiDGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Install the SDK and connect to Label Studio\n",
        "\n",
        "Let‚Äôs start by installing the SDK, configuring your connection details, and testing the connection.\n",
        "\n"
      ],
      "metadata": {
        "id": "vauOe_dOjiYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to install the Label Studio SDK\n",
        "!pip install label-studio-sdk>=2.0.11"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RmRELr5qkGoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure your connection details\n",
        "\n",
        "# URL of your Label Studio instance\n",
        "LABEL_STUDIO_URL = 'https://app.humansignal.com/'\n",
        "\n",
        "# Your API key (find it in Account & Settings > Personal Access Token)\n",
        "API_KEY = '<Token>'\n",
        "\n",
        "# ID of the workspace to create the project (make sure it's not the Personal Sandbox)\n",
        "# Find the Workspace ID in the URL when you click on it in the sidebar\n",
        "WORKSPACE_ID = 0"
      ],
      "metadata": {
        "id": "fAyS7DrKkIlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check you can connect to Label Studio\n",
        "from label_studio_sdk import LabelStudio\n",
        "\n",
        "ls = LabelStudio(base_url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
        "user = ls.users.whoami()\n",
        "print(f'Connected as {user.email}')"
      ],
      "metadata": {
        "id": "Jk-UY2eIv7fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Project and Import the Email Dataset\n",
        "\n",
        "1. We'll start by creating a **Label Studio project** with a labeling configuration that lets us classify each email as either **Phishing** or **Legitimate**.  \n",
        "   - If **Phishing** is selected, annotators can also choose one or more **phishing types** and **indicators** to describe the attack.\n",
        "\n",
        "2. Next, we‚Äôll **download a dataset of raw emails** that we‚Äôll use as the benchmark data.\n",
        "\n",
        "3. Finally, we‚Äôll **import the dataset as tasks into Label Studio**, so we can begin labeling and building our benchmark."
      ],
      "metadata": {
        "id": "bEoqDEhaw0kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_config = '''\n",
        "<View>\n",
        "  <View className=\"section\">\n",
        "    <Text name=\"body\" value=\"$raw_email\" />\n",
        "  </View>\n",
        "\n",
        "  <View className=\"section\">\n",
        "    <Header value=\"Overall classification\" size=\"3\" />\n",
        "    <Choices name=\"classification\" toName=\"body\" choice=\"single\" showInline=\"true\" required=\"true\">\n",
        "      <Choice value=\"Phishing\" />\n",
        "      <Choice value=\"Legitimate\" />\n",
        "    </Choices>\n",
        "  </View>\n",
        "\n",
        "  <View className=\"section\" visibleWhen=\"choice-selected\" whenTagName=\"classification\" whenChoiceValue=\"Phishing,Suspicious\">\n",
        "    <Header value=\"Phishing type\" size=\"3\" />\n",
        "    <Choices name=\"phishing_type\" toName=\"body\" choice=\"multiple\" showInline=\"false\">\n",
        "      <Choice value=\"Credential Harvesting - Fake login page\" />\n",
        "      <Choice value=\"Malware Delivery - Link to download malware\" />\n",
        "      <Choice value=\"Malware Delivery - Malicious Link / Phishing Site\" />\n",
        "      <Choice value=\"Financial Fraud - Gift Card Scam\" />\n",
        "      <Choice value=\"Financial Fraud - BEC (Impersonation/CEO Fraud)\" />\n",
        "      <Choice value=\"Extortion/Blackmail\" />\n",
        "      <Choice value=\"Account Suspension Notice\" />\n",
        "      <Choice value=\"Social Engineering - Romance / Dating Scam\" />\n",
        "      <Choice value=\"Other\" />\n",
        "    </Choices>\n",
        "\n",
        "    <Header value=\"Indicators observed\" size=\"3\" />\n",
        "    <Choices name=\"indicators\" toName=\"body\" choice=\"multiple\" showInline=\"false\">\n",
        "      <Choice value=\"Sender anomalies - Display name spoofing\" />\n",
        "      <Choice value=\"Sender anomalies - Lookalike domain\" />\n",
        "      <Choice value=\"Sender anomalies - Spoofed IP / Email headers indicate fraud\" />\n",
        "      <Choice value=\"Content signals - Urgency/Threats\" />\n",
        "      <Choice value=\"Content signals - Requests money/gift cards\" />\n",
        "      <Choice value=\"Content signals - Poor grammar/spelling\" />\n",
        "      <Choice value=\"Content signals - Unexpected attachment\" />\n",
        "      <Choice value=\"Content signals - Generic greeting (Dear user, etc.)\" />\n",
        "      <Choice value=\"Link issues - Obfuscated/shortened URL\" />\n",
        "      <Choice value=\"Link issues - Domain mismatch with brand\" />\n",
        "      <Choice value=\"Link issues - IP-based URL\" />\n",
        "      <Choice value=\"Other - Request for personal info unrelated to workflow\" />\n",
        "    </Choices>\n",
        "  </View>\n",
        "</View>\n",
        "'''\n",
        "# Creating project in the configured workspace\n",
        "project = ls.projects.create(\n",
        "    title='Phishing Benchmark Tutorial',\n",
        "    workspace=WORKSPACE_ID,\n",
        "    is_published=True,\n",
        "    label_config=label_config\n",
        ")"
      ],
      "metadata": {
        "id": "YQc5wuOFm8wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download emails dataset\n",
        "import pandas as pd\n",
        "DATASET_URL = 'https://gist.githubusercontent.com/mcanu/95507352bfc4557d0ff0430117758abe/raw/eb86433f4c40645d0b1b717877d3468456ab687a/raw_emails.csv'\n",
        "raw_emails = pd.read_csv(DATASET_URL)\n",
        "raw_emails.head()"
      ],
      "metadata": {
        "id": "5f6BvG4Gng1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import raw emails as tasks to Label Studio\n",
        "ls.projects.import_tasks(project.id, request=raw_emails.to_dict(orient='records'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k6k2Ydeo_kS",
        "outputId": "aed6b4df-b47c-49a4-881f-d65d34084b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProjectsImportTasksResponse(annotation_count=None, could_be_tasks_list=None, data_columns=None, duration=None, file_upload_ids=None, found_formats=None, predictions_count=None, task_count=None, import=3058032)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Your Benchmark\n",
        "\n",
        "Now, if you go to **Label Studio**, you should see your newly imported tasks in the **Data Manager**.  \n",
        "\n",
        "Click **Label All Tasks** to start labeling your benchmark.  \n",
        "These labeled tasks will serve as your **ground truth** ‚Äî the foundation for evaluating how well different models perform.  \n",
        "\n",
        "*(You can refer to the screenshots below to follow along.)*"
      ],
      "metadata": {
        "id": "QMODAgeFx3A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/HumanSignal/awesome-label-studio-tutorials/blob/main/tutorials/how-to-create-benchmark-and-evaluate-your-models/how_to_create_a_Benchmark_and_Evaluate_your_models_with_Label_Studio_files/figure_1.png?raw=true)"
      ],
      "metadata": {
        "id": "Uyf8JJilhPDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set Annotations as Ground Truth\n",
        "\n",
        "Mark your completed annotations as **ground truth**.  \n",
        "This tells Label Studio which annotations to treat as the **source of truth** ‚Äî enabling more accurate and meaningful **agreement metrics** when comparing model predictions against human labels."
      ],
      "metadata": {
        "id": "KZaXytmIyTXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your annotations as ground truth\n",
        "ls.actions.create(id='set_ground_truths', project=project.id, request_options={'additional_body_parameters': {'annotator': user.id}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad7I0IndyMS-",
        "outputId": "0e5555af-608d-470a-dab2-2606e46a8785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Prompts for Model Predictions\n",
        "\n",
        "**Prompts** let you connect and evaluate different AI models directly from Label Studio ‚Äî using a shared prompt template to generate consistent predictions.  \n",
        "\n",
        "Before running evaluations, go to [app.humansignal.com/prompts](https://app.humansignal.com/prompts) and configure the **API keys** for the models you want to test.  \n",
        "You can also add **custom model connections** to evaluate your own or third-party models.  \n",
        "\n",
        "For more details, see the [Prompts configuration guide](https://docs.humansignal.com/guide/prompts_keys).\n",
        "\n",
        "![image](https://github.com/HumanSignal/awesome-label-studio-tutorials/blob/main/tutorials/how-to-create-benchmark-and-evaluate-your-models/how_to_create_a_Benchmark_and_Evaluate_your_models_with_Label_Studio_files/figure_2.png?raw=true)"
      ],
      "metadata": {
        "id": "4mwbPIRIwj4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and Run Model Versions with the SDK\n",
        "\n",
        "Next, we‚Äôll use the **Label Studio SDK** to automate the process of model evaluation.  \n",
        "We‚Äôll create a **prompt** linked to our project, then generate a **model version** for each model we want to test.  \n",
        "\n",
        "Each model version will run inference on **all tasks in the project**, producing **one prediction per model per task** ‚Äî ready for comparison against the ground truth."
      ],
      "metadata": {
        "id": "kTbftyCQzHNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new prompt for our project\n",
        "prompt = ls.prompts.create(\n",
        "    title='Phishing Benchmark Tutorial Prompt',\n",
        "    associated_projects=[project.id],\n",
        "    input_fields=['raw_email'],\n",
        ")"
      ],
      "metadata": {
        "id": "7rrIS4PIwvTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be testing with 4 models from 2 different providers\n",
        "models = {\n",
        "    'OpenAI': {\n",
        "        'id': None,\n",
        "        'models': ['gpt-5-mini', 'gpt-4.1-mini'],\n",
        "      },\n",
        "    'Gemini': {\n",
        "        'id': None,\n",
        "        'models': ['gemini-2.0-flash-lite', 'gemini-2.5-flash-lite']\n",
        "    }\n",
        "}\n",
        "providers = ls.model_providers.list()\n",
        "\n",
        "# Assign provideer ids to models\n",
        "for provider in providers:\n",
        "    model_info = models.get(provider.provider)\n",
        "    if model_info:\n",
        "      model_info['id'] = provider.id\n",
        "      models[provider.provider] = model_info\n"
      ],
      "metadata": {
        "id": "uoMBBbgRDe2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = '''\n",
        "Analyze the provided email content and classify it as either 'Phishing' or 'Legitimate'.\n",
        "\n",
        "If the email is classified as 'Phishing', identify the specific phishing type(s) and indicators from the options.\n",
        "If the email is legitimate, classify it as 'Legitimate' with no additional phishing type or indicators.\n",
        "\n",
        "Use the following variable in your analysis:\n",
        "\"{raw_email}\"\n",
        "\n",
        "Provide the output in JSON format with the keys:\n",
        "- classification: 'Phishing' or 'Legitimate'\n",
        "- phishing_type: list of strings (if applicable)\n",
        "- indicators: list of strings (if applicable)\n",
        "\n",
        "Ensure the response adheres to the provided JSON schema.\n",
        "'''\n",
        "\n",
        "# Create one prompt version for each model\n",
        "versions = []\n",
        "for provider, info in models.items():\n",
        "    for model_name in info['models']:\n",
        "      version = ls.prompts.versions.create(\n",
        "          prompt_id=prompt.id,\n",
        "          prompt=PROMPT,\n",
        "          provider_model_id=model_name,\n",
        "          title=f'{provider} - {model_name}',\n",
        "          model_provider_connection=info['id'],\n",
        "\n",
        "      )\n",
        "      versions.append(version)\n"
      ],
      "metadata": {
        "id": "GHdYJDo-Auo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference for each version\n",
        "import time\n",
        "\n",
        "# Function to wait for runs to complete\n",
        "def wait_for_runs_to_complete(versions):\n",
        "  completed_versions = []\n",
        "  while len(completed_versions) != len(versions):\n",
        "    for version in versions:\n",
        "      if version in completed_versions:\n",
        "        continue\n",
        "      runs = ls.prompts.runs.list(prompt_id=prompt.id, version_id=version.id)\n",
        "      if runs:\n",
        "        last_run = runs[-1]\n",
        "        if last_run.status == 'Completed':\n",
        "          print('version ', version.id, ' completed')\n",
        "          completed_versions.append(version)\n",
        "    time.sleep(10)\n",
        "\n",
        "# Launch runs for all versions\n",
        "for version in versions:\n",
        "  ls.prompts.runs.create(prompt_id=prompt.id, version_id=version.id, project=project.id, project_subset='All')\n",
        "\n",
        "# Wait for runs to complete\n",
        "wait_for_runs_to_complete(versions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1RAKvRRS92v",
        "outputId": "28d5b9ab-efb8-4566-a330-01b4453e8520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "version  46550  completed\n",
            "version  46551  completed\n",
            "version  46552  completed\n",
            "version  46549  completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project = ls.projects.get(193733)\n",
        "prompt = ls.prompts.get(37050)\n",
        "versions = ls.prompts.versions.list(prompt_id=prompt.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ8YTtOH-r9j",
        "outputId": "7d84d163-af3c-495d-d6c6-d6ffde8f8aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=[], input_type=list])\n",
            "  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=90367, input_type=int])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collect Run Costs\n",
        "\n",
        "Let‚Äôs retrieve the **costs for each model run** to include them as additional data points.  \n",
        "Tracking run costs alongside performance metrics helps you understand the **trade-off between accuracy and efficiency** for each model."
      ],
      "metadata": {
        "id": "hpTGqys29Lba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect costs from each run\n",
        "costs = {}\n",
        "for version in versions:\n",
        "  runs = ls.prompts.runs.list(prompt_id=prompt.id, version_id=version.id)\n",
        "  last_run = runs[-1]\n",
        "  cost = ls.prompts.indicators.get(id=last_run.id, indicator_key='inference_cost')\n",
        "  costs[version.title] = {\n",
        "      'Total Cost': cost.values.get('total_cost'),\n",
        "      'Completions Token Count': cost.values.get('completion_tokens_count'),\n",
        "      'Prompt Token Count': cost.values.get('prompt_tokens_count'),\n",
        "  }"
      ],
      "metadata": {
        "id": "r0sJtkuX9euw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Use the existing costs dictionary to create the DataFrame\n",
        "cost_df = pd.DataFrame.from_dict(costs, orient='index')\n",
        "cost_df = cost_df.reset_index().rename(columns={'index': 'Model Version'})\n",
        "\n",
        "display(cost_df)"
      ],
      "metadata": {
        "id": "PR4c9IO4CKOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìà Analyze Results\n",
        "\n",
        "Now that all model predictions are complete, we can use the **agreement metrics** provided by Label Studio to measure performance.  \n",
        "\n",
        "Let‚Äôs start by looking at the **overall agreement** each model achieved against the ground truth annotations.  \n",
        "This metric shows the **percentage of agreement** between each model‚Äôs predictions and your benchmark labels ‚Äî giving a quick snapshot of model accuracy."
      ],
      "metadata": {
        "id": "1Ml_KIt4zp9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_version_name(prompt, version):\n",
        "  return f'{prompt.title}__{version.title}'\n",
        "\n",
        "\n",
        "gt_agreements = []\n",
        "for version in versions:\n",
        "  version_name = get_version_name(prompt, version)\n",
        "  agreement = ls.projects.stats.model_version_ground_truth_agreement(id=project.id, model_version=version_name)\n",
        "  gt_agreements.append(agreement)"
      ],
      "metadata": {
        "id": "B-3RipMMuSkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "324e2edb",
        "collapsed": true
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'versions' and 'prompt' are available from previous cells\n",
        "version_names = [version.title for version in versions]\n",
        "agreement_values = [item.agreement * 100 for item in gt_agreements]\n",
        "\n",
        "gt_agreement_df = pd.DataFrame({\n",
        "    'Model Version': version_names,\n",
        "    'Agreement (%)': agreement_values\n",
        "})\n",
        "\n",
        "gt_agreement_df = gt_agreement_df.sort_values('Agreement (%)', ascending=False)\n",
        "\n",
        "ax = gt_agreement_df.plot.bar(x='Model Version', y='Agreement (%)', legend=False, rot=45, color=plt.cm.viridis(gt_agreement_df.index / len(gt_agreement_df)))\n",
        "plt.ylabel('Agreement with Ground Truth (%)')\n",
        "plt.xlabel('Model Version')\n",
        "plt.title('Model Version Agreement with Ground Truth (Label Studio Stats)')\n",
        "\n",
        "# Add agreement percentages on top of the bars\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.1f%%')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agreement per Label\n",
        "\n",
        "Next, let‚Äôs explore the **agreement per label** to see how each model performed across different categories.  \n",
        "\n",
        "This view highlights where models **aligned with** or **deviated from** the benchmark ‚Äî both in the main email classification and in the specific **phishing types** and **indicators**.  \n",
        "\n",
        "These insights are extremely valuable for identifying **where models struggle** and **which areas to focus on** when fine-tuning to achieve better results."
      ],
      "metadata": {
        "id": "rDAursP40JLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agreement per label\n",
        "\n",
        "per_label_agreements = []\n",
        "for version in versions:\n",
        "  version_name = get_version_name(prompt, version)\n",
        "  agreement = ls.projects.stats.model_version_ground_truth_agreement(id=project.id, model_version=version_name, per_label=True)\n",
        "  per_label_agreements.append(agreement)"
      ],
      "metadata": {
        "id": "Ct_DPLQ61bd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4aa1344"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "per_label_data = {'Total Cost': {}}\n",
        "for i, version_agreement in enumerate(per_label_agreements):\n",
        "    version_name = versions[i].title\n",
        "    for label, agreement in version_agreement.agreement.items():\n",
        "        if label not in per_label_data:\n",
        "            per_label_data[label] = {}\n",
        "        per_label_data[label][version_name] = agreement * 100 # Convert to percentage\n",
        "    per_label_data['Total Cost'][version_name] = costs[version_name]['Total Cost']\n",
        "\n",
        "per_label_df = pd.DataFrame.from_dict(per_label_data, orient='index')\n",
        "per_label_df = per_label_df.fillna(0) # Fill missing values with 0 if a label doesn't appear for a model\n",
        "\n",
        "# Add color to the table and format percentages, excluding the 'Total Cost' row\n",
        "per_label_df.style.background_gradient(cmap='Pastel1', subset=pd.IndexSlice[per_label_df.index != 'Total Cost', :]).format(\"{:.1f}%\", subset=pd.IndexSlice[per_label_df.index != 'Total Cost', :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Wrapping Up\n",
        "\n",
        "Congrats ‚Äî you‚Äôve just built your own **benchmark** for phishing email classification using **Label Studio**!  \n",
        "\n",
        "With just a few steps, you created a setup that lets you:\n",
        "- üß† Evaluate how well an LLM classifies phishing vs. legitimate emails  \n",
        "- üìä Visualize and compare model outputs in a structured, repeatable way  \n",
        "- üîÑ Build a foundation that can evolve as your models improve  \n",
        "\n",
        "### üí° Why Benchmarks Matter\n",
        "\n",
        "Benchmarks are essential for understanding how your models behave ‚Äî not just whether they ‚Äúwork.‚Äù  \n",
        "They help you:\n",
        "- Identify **strengths and weaknesses** in model reasoning  \n",
        "- Track **progress over time** as your system improves  \n",
        "- Build **trust and transparency** around performance  \n",
        "\n",
        "Benchmarks aren‚Äôt static ‚Äî they evolve.  \n",
        "You can continuously expand this benchmark with new examples or categories as your needs grow.\n",
        "\n",
        "### üß© Why Label Studio\n",
        "\n",
        "Label Studio makes this process simple and collaborative:\n",
        "- Create, manage, and version **custom benchmarks**  \n",
        "- **Evaluate multiple models** on the same dataset  \n",
        "- **Visualize metrics and insights** directly from your labeled data  \n",
        "- Integrate results into your existing ML pipeline for continuous improvement  \n",
        "\n",
        "That‚Äôs exactly how teams like [*LegalBenchmarks.AI*](https://labelstud.io/blog/how-legalbenchmarks-ai-built-a-domain-specific-ai-benchmark/) build reliable, evolving benchmarks across domains.  \n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "Keep iterating on what you built today:\n",
        "- üß™ Add more data or expand your label schema  \n",
        "- ü§ñ Compare different LLMs or prompt variations  \n",
        "- üìà Analyze where your model struggles ‚Äî and why  \n",
        "- üîó Share or automate your evaluation process with your team\n",
        "\n",
        "### üìö Learn More\n",
        "\n",
        "- [Label Studio Documentation](https://docs.humansignal.com/guide/)  \n",
        "- [Label Studio SDK](https://humansignal.github.io/label-studio-sdk/)  \n",
        "- [Blog: Why Benchmarks Matter for Evaluating LLMs](https://labelstud.io/blog/why-benchmarks-matter-for-evaluating-llms/)  \n",
        "- [Blog: How to Build AI Benchmarks That Evolve with Your Models](https://labelstud.io/blog/how-to-build-ai-benchmarks-that-evolve-with-your-models/)  \n",
        "- [Blog: Evaluating the GPT-5 Series on Custom Benchmarks](https://labelstud.io/blog/evaluating-the-gpt-5-series-on-custom-benchmarks/)  \n",
        "- [Blog: How LegalBenchmarks.AI Built a Domain-Specific AI Benchmark](https://labelstud.io/blog/how-legalbenchmarks-ai-built-a-domain-specific-ai-benchmark/)  "
      ],
      "metadata": {
        "id": "V6ppMctxnumx"
      }
    }
  ]
}