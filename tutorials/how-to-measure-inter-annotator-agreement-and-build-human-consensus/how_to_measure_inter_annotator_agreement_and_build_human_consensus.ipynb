{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6f030bd-a254-418b-8e0f-04a5bbca697f",
      "metadata": {
        "id": "f6f030bd-a254-418b-8e0f-04a5bbca697f"
      },
      "source": [
        "This tutorial walks through a practical workflow to measure inter-annotator agreement, build human consensus, establish ground truth and\n",
        "compare model predictions to that consensus using the Label Studio SDK.\n",
        "\n",
        "Being able to measure agreement between different annotators, ground truth and models can help you get a sense of annotator/model performance, and help you make informed decisions to ultimately get higher quality labels in your project.\n",
        "\n",
        "**Why This Matters**\n",
        "- Reduce ambiguity: Low agreement reveals unclear instructions and subjective edge cases to fix.\n",
        "- Improve label quality: Consensus strength is a practical confidence signal for ground truth.\n",
        "- Target QA: Focus review on contentious tasks and annotator pairs instead of boiling the ocean.\n",
        "- Fair model eval: Compare models to human consensus, not a single rater, to avoid evaluator bias.\n",
        "- Faster iteration: Quantify reliability, prioritize fixes, and measure improvements over time.\n",
        "\n",
        "**In this tutorial you will**\n",
        "- Import data with overlap and (for demo) simulate multiple annotators.\n",
        "- Compute IAA and per‑task consensus (majority vote) to establish ground truth.\n",
        "- Compare a model to human consensus, flag low‑agreement and disagreement items, and bulk‑assign them for QA.\n",
        "- Visualize results and summarize reliability to inform next steps.\n",
        "\n",
        "**Outputs**\n",
        "- Per‑task consensus + agreement ratio (confidence proxy)\n",
        "- Model alignment score vs human consensus\n",
        "- Targeted QA task lists for re‑annotation/review\n",
        "\n",
        "**Notes:**\n",
        "- We provide steps here to create a dataset from scratch, but you can skip these steps and use an existing project as well.\n",
        "- Fill in your Label Studio URL and API key.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Studio Requirements\n",
        "\n",
        "This tutorial showcases one or more features available only in Label Studio paid products. We recommend [creating a Starter Cloud trial](https://app.humansignal.com/user/cloud-trial?offer=d9a5&) to follow the tutorial."
      ],
      "metadata": {
        "id": "0zmpOQyK21aw"
      },
      "id": "0zmpOQyK21aw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "AhWL7RYOA4t9"
      },
      "id": "AhWL7RYOA4t9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4312d22-a77d-464e-8e33-324a0d1b0d7a",
      "metadata": {
        "id": "b4312d22-a77d-464e-8e33-324a0d1b0d7a"
      },
      "outputs": [],
      "source": [
        "!pip install -q label-studio-sdk pandas numpy seaborn scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba4e265-51a1-4c0a-9e73-a09557a24429",
      "metadata": {
        "id": "6ba4e265-51a1-4c0a-9e73-a09557a24429"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "796126dd-85d5-43dc-b29d-44dd7ff0547a",
      "metadata": {
        "id": "796126dd-85d5-43dc-b29d-44dd7ff0547a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from label_studio_sdk import LabelStudio\n",
        "from label_studio_sdk.types import ImportApiRequest, PredictionRequest\n",
        "from label_studio_sdk.projects.assignments import AssignmentsBulkAssignRequestSelectedItemsIncluded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "419017bd-bfa8-43ee-8b69-940cbe08716a",
      "metadata": {
        "id": "419017bd-bfa8-43ee-8b69-940cbe08716a"
      },
      "source": [
        "## 1) Connect to Label Studio\n",
        "\n",
        "Replace placeholders with your details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989d8134-188b-472c-bd24-b5dddbab654e",
      "metadata": {
        "id": "989d8134-188b-472c-bd24-b5dddbab654e"
      },
      "outputs": [],
      "source": [
        "# URL of your Label Studio instance\n",
        "BASE_URL = \"https://app.humansignal.com\"\n",
        "\n",
        "# Your API key (find it in Account & Settings > Personal Access Token)\n",
        "API_KEY = \"YOUR API KEY\"\n",
        "\n",
        "ls = LabelStudio(base_url=BASE_URL, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1f93b9d-b955-4e8c-b4c0-0f1febb04e7e",
      "metadata": {
        "id": "f1f93b9d-b955-4e8c-b4c0-0f1febb04e7e"
      },
      "source": [
        "## 2) Create a Project (or use existing)\n",
        "\n",
        "We'll create a project with subjective tasks (toxicity severity) that benefits from multiple human judgments.\n",
        "You can swap this labeling config for another subjective task, e.g., sarcasm detection, humor rating, or topic relevance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f390a176-5c6f-42f7-8c82-733f581e1eb1",
      "metadata": {
        "id": "f390a176-5c6f-42f7-8c82-733f581e1eb1"
      },
      "outputs": [],
      "source": [
        "LABEL_CONFIG = \"\"\"\n",
        "<View>\n",
        "  <Text name=\"text\" value=\"$text\"/>\n",
        "  <Choices name=\"toxicity\" toName=\"text\" choice=\"single\" showInLine=\"true\">\n",
        "    <Choice value=\"Non-toxic\"/>\n",
        "    <Choice value=\"Somewhat toxic\"/>\n",
        "    <Choice value=\"Very toxic\"/>\n",
        "  </Choices>\n",
        "  <Style> .lsf-labels .lsf-choices__item { padding: 6px 10px; } </Style>\n",
        "  <Choices name=\"flag\" toName=\"text\" choice=\"single\" showInLine=\"true\" visibleWhen=\"region-selected=false\" perRegion=\"false\">\n",
        "    <Choice value=\"Needs QA\"/>\n",
        "  </Choices>\n",
        "</View>\n",
        "\"\"\"\n",
        "\n",
        "PROJECT_TITLE = \"Agreement & Consensus Tutorial\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12422691-7377-4b94-847a-3cfd8cbf294a",
      "metadata": {
        "id": "12422691-7377-4b94-847a-3cfd8cbf294a"
      },
      "source": [
        "### Create Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20be1d5e-91af-4c53-b46a-a2f364fe1a93",
      "metadata": {
        "id": "20be1d5e-91af-4c53-b46a-a2f364fe1a93"
      },
      "outputs": [],
      "source": [
        "project = ls.projects.create(\n",
        "    title=PROJECT_TITLE,\n",
        "    label_config=LABEL_CONFIG,\n",
        "    description=(\n",
        "        \"Subjective toxicity severity labeling with multiple annotators to measure agreement and build consensus.\"\n",
        "    ),\n",
        "    show_collab_predictions=True,\n",
        "    maximum_annotations=3,  # allow up to 3 annotations per task\n",
        "    is_published=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ddbe498-1ea5-4a16-990c-741d9e6b88bd",
      "metadata": {
        "id": "4ddbe498-1ea5-4a16-990c-741d9e6b88bd"
      },
      "source": [
        "### Alternatively Get Project by ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ecd232-7d85-4db1-899d-e4392d31cf77",
      "metadata": {
        "id": "80ecd232-7d85-4db1-899d-e4392d31cf77"
      },
      "outputs": [],
      "source": [
        "# Uncomment and replace project ID with the one you wish to use\n",
        "# project = ls.projects.get(id=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e01ea24-1564-46b2-8c08-d5002bdff2aa",
      "metadata": {
        "id": "7e01ea24-1564-46b2-8c08-d5002bdff2aa"
      },
      "source": [
        "### Confirm We Are Using Correct Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65995994-3a80-4580-bc69-e43eee3ef7ee",
      "metadata": {
        "id": "65995994-3a80-4580-bc69-e43eee3ef7ee"
      },
      "outputs": [],
      "source": [
        "print(f\"Using project ID {project.id}: {project.title}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb07c44f-bd49-42ee-baef-90dc5cfac6a6",
      "metadata": {
        "id": "cb07c44f-bd49-42ee-baef-90dc5cfac6a6"
      },
      "source": [
        "## 3) Import Data (If creating new project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e481ee6e-8cf3-4484-8fe4-ec58d7b71eaa",
      "metadata": {
        "id": "e481ee6e-8cf3-4484-8fe4-ec58d7b71eaa"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "    \"I can't believe you think that's acceptable.\",\n",
        "    \"Great job on the release, really proud of the team!\",\n",
        "    \"You're clueless.\"\n",
        "    \"\",\n",
        "    \"This comment is borderline rude, but maybe not intended.\",\n",
        "    \"That post was hilarious, I loved it.\",\n",
        "    \"This is the worst idea I've ever heard.\",\n",
        "    \"Please refrain from using that tone here.\",\n",
        "    \"I appreciate your perspective though we disagree.\",\n",
        "    \"What a mess; whoever wrote this didn't think it through.\",\n",
        "]\n",
        "\n",
        "tasks = [{\"data\": {\"text\": t}} for t in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9165de-1756-4e33-8447-b8daa6c9b92a",
      "metadata": {
        "id": "ea9165de-1756-4e33-8447-b8daa6c9b92a"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.import_tasks(\n",
        "    id=project.id,\n",
        "    request=tasks,\n",
        "    commit_to_project=True,\n",
        "    return_task_ids=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb507fea-d7e3-4858-94d4-b6670de35dd8",
      "metadata": {
        "id": "cb507fea-d7e3-4858-94d4-b6670de35dd8"
      },
      "source": [
        "### Ensure tasks were created\n",
        "If importing data from above, there should be 9 tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b71fe53-fe1c-4128-993a-a13ff5135024",
      "metadata": {
        "id": "1b71fe53-fe1c-4128-993a-a13ff5135024"
      },
      "outputs": [],
      "source": [
        "tasks = [t for t in ls.tasks.list(project=project.id)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a3c57b-3a4f-4c63-8358-bbc4bd2cd627",
      "metadata": {
        "id": "74a3c57b-3a4f-4c63-8358-bbc4bd2cd627"
      },
      "outputs": [],
      "source": [
        "print(f\"There are {len(tasks)} tasks in project {project.title}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e52d2f-27c0-4a1b-8293-142b1bb8729d",
      "metadata": {
        "id": "48e52d2f-27c0-4a1b-8293-142b1bb8729d"
      },
      "source": [
        "## 4) Create or Get Annotators"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd95f84-5359-45f8-b364-4a3da52177b1",
      "metadata": {
        "id": "efd95f84-5359-45f8-b364-4a3da52177b1"
      },
      "source": [
        "### Create Annotator Users if Needed\n",
        "If you have annotations already in your project, skip this step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa4b024-afa6-437a-b638-678d27e58f13",
      "metadata": {
        "id": "cfa4b024-afa6-437a-b638-678d27e58f13"
      },
      "outputs": [],
      "source": [
        "current_user = ls.users.get_current_user()\n",
        "org = current_user.active_organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878a5650-6f24-4066-8f1b-acd0aba14921",
      "metadata": {
        "id": "878a5650-6f24-4066-8f1b-acd0aba14921"
      },
      "outputs": [],
      "source": [
        "annotator1 = ls.users.create(first_name=\"Annotator\", last_name=\"1\", username=\"Annotator1\", email=\"annotator1@mycompany.com\")\n",
        "annotator2 = ls.users.create(first_name=\"Annotator\", last_name=\"2\", username=\"Annotator2\", email=\"annotator2@mycompany.com\")\n",
        "annotator3 = ls.users.create(first_name=\"Annotator\", last_name=\"3\", username=\"Annotator3\", email=\"annotator3@mycompany.com\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61141cf-ec6a-4d75-b631-715eace6772b",
      "metadata": {
        "id": "a61141cf-ec6a-4d75-b631-715eace6772b"
      },
      "outputs": [],
      "source": [
        "ls.organizations.members.update(id=org, user_id=annotator1.id, role=\"AN\")\n",
        "ls.organizations.members.update(id=org, user_id=annotator2.id, role=\"AN\")\n",
        "ls.organizations.members.update(id=org, user_id=annotator3.id, role=\"AN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d2aed2-7e6e-49cd-8a75-e462a629f47e",
      "metadata": {
        "id": "f4d2aed2-7e6e-49cd-8a75-e462a629f47e"
      },
      "source": [
        "### Alternatively Get Existing Annotators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8aa5100-40ca-4384-ac33-2c052d5b5e9c",
      "metadata": {
        "id": "e8aa5100-40ca-4384-ac33-2c052d5b5e9c"
      },
      "outputs": [],
      "source": [
        "# Replace user IDs with annotators from your organization\n",
        "annotator1 = ls.users.get(id=31228)\n",
        "annotator2 = ls.users.get(id=13061)\n",
        "annotator3 = ls.users.get(id=11551)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10a9dfee-9c26-49f9-80e3-c003ae029d79",
      "metadata": {
        "id": "10a9dfee-9c26-49f9-80e3-c003ae029d79"
      },
      "source": [
        "## 5) Create Annotations for Tasks\n",
        "This will create 3 annotations per task in the project. This will simulate strong consensus on some tasks, and disagreement on others. <br>\n",
        "__* Skip this step if you are using an existing project that already has annotations.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e847a096-aa37-4112-8d0a-dd3767085de4",
      "metadata": {
        "id": "e847a096-aa37-4112-8d0a-dd3767085de4"
      },
      "outputs": [],
      "source": [
        "annotator_ids = [annotator1.id, annotator2.id, annotator3.id]\n",
        "# Will be used in agreement matrix step\n",
        "annotator_id_to_initials = {annotator1.id: annotator1.initials, annotator2.id: annotator2.initials, annotator3.id: annotator3.initials}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6508269d-c21a-4150-8b1b-7d3d11577ce1",
      "metadata": {
        "id": "6508269d-c21a-4150-8b1b-7d3d11577ce1"
      },
      "outputs": [],
      "source": [
        "def to_annotation_result(label: str, from_name: str = \"toxicity\", to_name: str = \"text\") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Helper function that takes a single label and returns a full result dict\n",
        "    \"\"\"\n",
        "    return [\n",
        "        {\n",
        "            \"from_name\": from_name,\n",
        "            \"to_name\": to_name,\n",
        "            \"type\": \"choices\",\n",
        "            \"value\": {\"choices\": [label]},\n",
        "        }\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfaafcda-3f66-4a8b-bfff-aac3218d6b1f",
      "metadata": {
        "id": "bfaafcda-3f66-4a8b-bfff-aac3218d6b1f"
      },
      "outputs": [],
      "source": [
        "for idx, task in enumerate(tasks):\n",
        "    # Define label pattern by index\n",
        "    if idx % 3 == 0:\n",
        "        # strong consensus\n",
        "        labels = [\"Non-toxic\", \"Non-toxic\", \"Non-toxic\"]\n",
        "    elif idx % 3 == 1:\n",
        "        # no consensus (all different)\n",
        "        labels = [\"Non-toxic\", \"Somewhat toxic\", \"Very toxic\"]\n",
        "    else:\n",
        "        # weak consensus (2 vs 1)\n",
        "        labels = [\"Somewhat toxic\", \"Very toxic\", \"Very toxic\"]\n",
        "\n",
        "    for i, lab in enumerate(labels):\n",
        "        kwargs: Dict[str, Any] = {\"result\": to_annotation_result(lab)}\n",
        "        if annotator_ids:\n",
        "            kwargs[\"completed_by\"] = annotator_ids[i % len(annotator_ids)]\n",
        "        ls.annotations.create(id=task.id, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae60d1c-0f73-4ba4-a4ee-e3c4c6a748a9",
      "metadata": {
        "id": "9ae60d1c-0f73-4ba4-a4ee-e3c4c6a748a9"
      },
      "source": [
        "## 6) Measure and Analyze Agreement\n",
        "Label Studio Starter Cloud and Label Studio Enterprise provide a variety of features that allow you to see agreement at different levels. We will start with project level stats and work our way down into more detailed stats."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc10077e-78ab-4343-8e40-f2d7573110d9",
      "metadata": {
        "id": "dc10077e-78ab-4343-8e40-f2d7573110d9"
      },
      "source": [
        "### Get Project-Level Agreement\n",
        "This is the average agreement score across all tasks in the project <br>\n",
        "Gives you a sense of overall agreement at a high level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d9eb4b6-07e8-4bf9-800e-4fc57beef8c5",
      "metadata": {
        "id": "0d9eb4b6-07e8-4bf9-800e-4fc57beef8c5"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.total_agreement(id=project.id)\n",
        "print(f\"Total Agreement for Project: {response.total_agreement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32135001-dd6e-4ad5-9a92-7de07be5b0cb",
      "metadata": {
        "id": "32135001-dd6e-4ad5-9a92-7de07be5b0cb"
      },
      "source": [
        "### Get Annotator-Level Agreement\n",
        "This is the average agreement score across all tasks, for a specific annotator. <br>\n",
        "This can provide you information on how well a single annotator is agreeing with others in the project. <br>\n",
        "For example here we can see annotator 2 has a higher agreement across all other annotators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4425ce40-2bd2-4819-a08f-8ae723c81dfb",
      "metadata": {
        "id": "4425ce40-2bd2-4819-a08f-8ae723c81dfb"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.agreement_annotator(id=project.id, user_id=annotator1.id)\n",
        "print(f\"Annotator Agreement for {annotator1.email}: {response.agreement_per_annotator*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ccd3d5-8dd9-4a13-a375-ee7c7c54ef8f",
      "metadata": {
        "id": "a5ccd3d5-8dd9-4a13-a375-ee7c7c54ef8f"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.agreement_annotator(id=project.id, user_id=annotator2.id)\n",
        "print(f\"Annotator Agreement for {annotator2.email}: {response.agreement_per_annotator*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a619ee1-bbf1-4a63-84c8-e928c5687ffa",
      "metadata": {
        "id": "0a619ee1-bbf1-4a63-84c8-e928c5687ffa"
      },
      "source": [
        "### Annotator Agreement Matrix\n",
        "Using the IAA (inter-annotator-agreement) we can get a matrix of annotators in the project, and see how specific annotators agree with eachother."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211f47d9-1edf-466c-99f8-b750c880ac21",
      "metadata": {
        "id": "211f47d9-1edf-466c-99f8-b750c880ac21"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.iaa(id=project.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c8d90a-8b5b-485f-ac31-d51f84d6126b",
      "metadata": {
        "id": "94c8d90a-8b5b-485f-ac31-d51f84d6126b"
      },
      "outputs": [],
      "source": [
        "# Plot the matrix as a table\n",
        "iaa_matrix = np.array(response.iaa, dtype=float)\n",
        "user_labels = [annotator_id_to_initials[u] for u in (response.users or list(range(1, iaa_matrix.shape[0] + 1)))]\n",
        "\n",
        "# Create percentage text, hide diagonal (self-agreement)\n",
        "cell_text = []\n",
        "for i in range(iaa_matrix.shape[0]):\n",
        "    row = []\n",
        "    for j in range(iaa_matrix.shape[1]):\n",
        "        row.append(\"—\" if i == j else f\"{iaa_matrix[i, j] * 100:.0f}%\")\n",
        "    cell_text.append(row)\n",
        "\n",
        "fig_w = 1.5 + 0.9 * len(user_labels)\n",
        "fig_h = 1.2 + 0.7 * len(user_labels)\n",
        "fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
        "ax.axis(\"off\")\n",
        "\n",
        "table = ax.table(\n",
        "    cellText=cell_text,\n",
        "    rowLabels=user_labels,\n",
        "    colLabels=user_labels,\n",
        "    cellLoc=\"center\",\n",
        "    loc=\"center\",\n",
        ")\n",
        "table.auto_set_font_size(True)\n",
        "#table.set_fontsize(15)\n",
        "table.scale(1.5, 2)\n",
        "\n",
        "ax.set_title(\"Inter-Annotator Agreement (IAA)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e1071e-2289-4f24-b99a-5a42b713ddf2",
      "metadata": {
        "id": "d0e1071e-2289-4f24-b99a-5a42b713ddf2"
      },
      "source": [
        "Here we can see how differerent annotators agree with eachother, which annotators have a stronger consensus versus which annotators do not. This can give you an idea of annotator performance and allows you make decisions based on this data, such as removing certain annotators from a project, or assigning more tasks to annotators with high agreement."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Route Low-Agreement Tasks for Further Annotation\n",
        "Use task-level agreement values to identify items below a configurable threshold and bulk-assign them for additional annotations (AN). This is useful to strengthen consensus on ambiguous items."
      ],
      "metadata": {
        "id": "2vIsSgU7B8E7"
      },
      "id": "2vIsSgU7B8E7"
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = ls.tasks.list(project=project.id)"
      ],
      "metadata": {
        "id": "FPGXh1zpCDah"
      },
      "id": "FPGXh1zpCDah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a threshold for low agreement to filter tasks that we want to re-annotate - 50% for this example but in a real world project it would likely be higher\n",
        "low_agreement_threshold = 0.5"
      ],
      "metadata": {
        "id": "fA6O1XdhF5Im"
      },
      "id": "fA6O1XdhF5Im",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks_to_reannotate = []\n",
        "for task in tasks:\n",
        "    if task.agreement < low_agreement_threshold:\n",
        "        tasks_to_reannotate.append(task.id)"
      ],
      "metadata": {
        "id": "SiAF1KITGQnn"
      },
      "id": "SiAF1KITGQnn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: This bulk assign call requires that the user we are assigning to has signed in and used the platform. If you are using the programmatic flow from above to create users, we can only assign to the current_user. If you retrieved existing annotators, we can assign to one of those (annotator1, annotator2, annotator3) <br>\n",
        "To make sure this call works for everyone using this tutorial, we will assign them to the current user"
      ],
      "metadata": {
        "id": "9Y9dAqhQK8Lg"
      },
      "id": "9Y9dAqhQK8Lg"
    },
    {
      "cell_type": "code",
      "source": [
        "current_user_id = ls.users.get_current_user().id\n",
        "selected = AssignmentsBulkAssignRequestSelectedItemsIncluded(all_=False, included=tasks_to_reannotate)\n",
        "resp = ls.projects.assignments.bulk_assign(\n",
        "    id=project.id,\n",
        "    type=\"AN\",  # assign for additional annotation , also possible to assign for review using \"RE\"\n",
        "    users=[current_user_id],\n",
        "    selected_items=selected,\n",
        ")"
      ],
      "metadata": {
        "id": "RuYpLhauKwtn"
      },
      "id": "RuYpLhauKwtn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "071a275a-496a-44cb-a499-3cc6c463f0ec",
      "metadata": {
        "id": "071a275a-496a-44cb-a499-3cc6c463f0ec"
      },
      "source": [
        "## 8) Get Task-Level Agreement and Establish Ground Truth\n",
        "We can get agreement on a task-level which will help us establish ground truth based on annotator consensus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec838a9-bcdc-4787-b0d8-8280713b3b11",
      "metadata": {
        "id": "fec838a9-bcdc-4787-b0d8-8280713b3b11"
      },
      "outputs": [],
      "source": [
        "tasks = ls.tasks.list(project=project.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b78abb-9938-455b-aa42-2d5f9cbee88f",
      "metadata": {
        "id": "d2b78abb-9938-455b-aa42-2d5f9cbee88f"
      },
      "outputs": [],
      "source": [
        "# Configure threshold to consider a task as high consensus and add an annotation as ground truth - for this example we will use 100%\n",
        "gt_threshold = 1.0\n",
        "for task in tasks:\n",
        "    if task.agreement >= gt_threshold:\n",
        "        annotations = ls.annotations.list(id=task.id)\n",
        "        # Add new GT annotation so we can compare each annotator to GT individually\n",
        "        ls.annotations.create(id=task.id, result=annotations[0].result, ground_truth=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c02d2d-795e-47df-b27f-bc0e65707132",
      "metadata": {
        "id": "e1c02d2d-795e-47df-b27f-bc0e65707132"
      },
      "source": [
        "## 9) Annotator-Ground Truth Agreement\n",
        "However you choose to add ground truth to your project - comparing annotators against ground truth allows you to really get an understanding of annotator performance. <br>\n",
        "Comparing an annotator versus other annotators can give you an idea of their performance, but comparing against ground truth is one of the most valuable metrics for an annotator. <br>\n",
        "For example this can inform you on which annotators might need more training or education on how to properly label tasks in this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc40087-f154-479a-b348-2e595e67274c",
      "metadata": {
        "id": "8cc40087-f154-479a-b348-2e595e67274c"
      },
      "outputs": [],
      "source": [
        "# Ensure we have the latest stats calculated\n",
        "response = ls.projects.stats.update_stats(id=project.id, stat_type=\"stats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f441f03-0c66-4be4-8c6f-645ef7f85aab",
      "metadata": {
        "id": "2f441f03-0c66-4be4-8c6f-645ef7f85aab"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.user_ground_truth_agreement(id=project.id, user_pk=annotator1.id)\n",
        "print(f\"Annotator-Ground Truth Agreement for {annotator1.email}: {response.agreement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c65b4b2-490a-4626-8149-4e53bfac4aa2",
      "metadata": {
        "id": "6c65b4b2-490a-4626-8149-4e53bfac4aa2"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.user_ground_truth_agreement(id=project.id, user_pk=annotator2.id)\n",
        "print(f\"Annotator-Ground Truth Agreement for {annotator2.email}: {response.agreement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34ea219-a6b0-4880-9500-fb9b0e337044",
      "metadata": {
        "id": "a34ea219-a6b0-4880-9500-fb9b0e337044"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.user_ground_truth_agreement(id=project.id, user_pk=annotator3.id)\n",
        "print(f\"Annotator-Ground Truth Agreement for {annotator3.email}: {response.agreement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d981f662-0991-49ba-a48e-f8d9b58ef421",
      "metadata": {
        "id": "d981f662-0991-49ba-a48e-f8d9b58ef421"
      },
      "source": [
        "## 10) Model-Annotator Agreement\n",
        "Comparing model predictions against annotators and/or ground truth is a useful way to see how well your model is performing. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7c7a9e6-c381-47c2-995b-dceae113c68a",
      "metadata": {
        "id": "c7c7a9e6-c381-47c2-995b-dceae113c68a"
      },
      "source": [
        "### Add Predictions\n",
        "There are many ways to add predictions to a project in Label Studio, such as connecting a model, using prompts, or importing them manually. <br>\n",
        "For this example we will import them directly into the project.\n",
        "We will add one prediction per task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb97678-b03d-45cd-a7e1-8b73728c955d",
      "metadata": {
        "id": "beb97678-b03d-45cd-a7e1-8b73728c955d"
      },
      "outputs": [],
      "source": [
        "# Get all tasks\n",
        "tasks = ls.tasks.list(project=project.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c5cc1e-814f-4f85-b559-63af8be6c4d1",
      "metadata": {
        "id": "a9c5cc1e-814f-4f85-b559-63af8be6c4d1"
      },
      "outputs": [],
      "source": [
        "# Possible predictions\n",
        "labels = [\"Non-toxic\", \"Somewhat toxic\", \"Very toxic\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fee7628-8df1-4291-a3fb-e2f495a867a3",
      "metadata": {
        "id": "2fee7628-8df1-4291-a3fb-e2f495a867a3"
      },
      "outputs": [],
      "source": [
        "model_version1 = \"model-v1\"\n",
        "model_version2 = \"model-v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668d6d31-98d5-43aa-b19f-68798a611834",
      "metadata": {
        "id": "668d6d31-98d5-43aa-b19f-68798a611834"
      },
      "outputs": [],
      "source": [
        "# For each task, create a prediction from model-v1 , always outputting \"Non-Toxic\"\n",
        "for task in tasks:\n",
        "    ls.predictions.create(\n",
        "        task=task.id,\n",
        "        model_version=model_version1,\n",
        "        result=to_annotation_result(labels[0])\n",
        "    )\n",
        "\n",
        "# For each task, create a prediction from model-v2 , always outputting \"Somewhat toxic\"\n",
        "for task in tasks:\n",
        "    ls.predictions.create(\n",
        "        task=task.id,\n",
        "        model_version=model_version2,\n",
        "        result=to_annotation_result(labels[2])\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ee533d-1df7-415e-bf80-1262b117311f",
      "metadata": {
        "id": "86ee533d-1df7-415e-bf80-1262b117311f"
      },
      "source": [
        "### Get Agreement Between Model and Ground Truth\n",
        "Helps you easily understand how your model is performing against ground truth tasks <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146437d9-a407-466d-80ab-35657744027d",
      "metadata": {
        "id": "146437d9-a407-466d-80ab-35657744027d"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.model_version_ground_truth_agreement(id=project.id, model_version=model_version1)\n",
        "print(f\"Agreement between {model_version1} and ground truth: {response.agreement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15ea3e2-b19f-49d6-ae7c-bb69bb32cb1b",
      "metadata": {
        "id": "b15ea3e2-b19f-49d6-ae7c-bb69bb32cb1b"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.model_version_ground_truth_agreement(id=project.id, model_version=model_version2)\n",
        "print(f\"Agreement between {model_version2} and ground truth: {response.agreement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d69b90d-860a-484c-8cbf-209867a8dbc3",
      "metadata": {
        "id": "4d69b90d-860a-484c-8cbf-209867a8dbc3"
      },
      "source": [
        "### Get Agreement Between Model and All Annotators\n",
        "Helps you get a sense of how well your model is agreeing with your annotators <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5d29fe-f6e0-450e-b5c7-5885cf05baec",
      "metadata": {
        "id": "9d5d29fe-f6e0-450e-b5c7-5885cf05baec"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.model_version_annotator_agreement(id=project.id, model_version=model_version1)\n",
        "print(f\"Agreement between {model_version1} and all annotators: {response.agreement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ded22ec0-b4b3-4e37-994e-9e2958358b07",
      "metadata": {
        "id": "ded22ec0-b4b3-4e37-994e-9e2958358b07"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.model_version_annotator_agreement(id=project.id, model_version=model_version2)\n",
        "print(f\"Agreement between {model_version2} and all annotators: {response.agreement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e435b3c1-f85b-4e89-aec5-786d282ad414",
      "metadata": {
        "id": "e435b3c1-f85b-4e89-aec5-786d282ad414"
      },
      "source": [
        "### Get Agreement Between One Model and All Other Models\n",
        "Helps you understand agreement between models <br>\n",
        "In this case we created predictions from 2 different models, with both of them outputting different labels per-task so agreement will be 0%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f92894f-0562-4b02-9ad6-0f9e12847ba0",
      "metadata": {
        "id": "3f92894f-0562-4b02-9ad6-0f9e12847ba0"
      },
      "outputs": [],
      "source": [
        "response = ls.projects.stats.model_version_prediction_agreement(id=project.id, model_version=model_version1)\n",
        "print(f\"Agreement between {model_version1} and all other models: {response.average_prediction_agreement_per_model*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "My Virtual Environment (Python 3.x)",
      "language": "python",
      "name": "my_venv_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}