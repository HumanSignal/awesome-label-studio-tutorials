{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ccb84b2b",
      "metadata": {
        "id": "ccb84b2b"
      },
      "source": [
        "## 1. Installation & Setup\n",
        "\n",
        "First, install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b8f4db",
      "metadata": {
        "collapsed": true,
        "id": "82b8f4db"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-openai langchain-community langsmith label-studio-sdk python-dotenv arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eba01bea",
      "metadata": {
        "id": "eba01bea"
      },
      "source": [
        "### Environment Configuration\n",
        "\n",
        "You'll need to configure the following environment variables in a `.env` file:\n",
        "\n",
        "```bash\n",
        "# Langsmith Configuration\n",
        "# Get your API key and setup project at: https://docs.langchain.com/langsmith/home\n",
        "LANGSMITH_API_KEY=your_langsmith_api_key\n",
        "LANGSMITH_PROJECT=your_project_name\n",
        "LANGSMITH_TRACING=true\n",
        "\n",
        "# Label Studio Configuration  \n",
        "# Get your API key from Label Studio settings: https://docs.humansignal.com/\n",
        "LS_URL=https://app.humansignal.com  # or your Label Studio instance URL\n",
        "LS_API_KEY=your_label_studio_api_key\n",
        "\n",
        "# OpenAI Configuration (for the agent)\n",
        "OPENAI_API_KEY=your_openai_api_key\n",
        "```\n",
        "\n",
        "**Langsmith Setup**: Visit [Langsmith Documentation](https://docs.langchain.com/langsmith/home)\n",
        "to create an account, get your API key, and set up a project for tracing.\n",
        "\n",
        "**Label Studio Setup**: Visit [Label Studio Documentation](https://docs.humansignal.com/)\n",
        "for installation instructions and how to generate an API token from your account settings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296d7ea1",
      "metadata": {
        "id": "296d7ea1"
      },
      "source": [
        "# Two Ways to Look at Your Data in Agentic Apps\n",
        "\n",
        " | üëÅÔ∏è AI Engineer | üëÅÔ∏è AI Domain Expert |\n",
        "|-------------|------------------|\n",
        "| <img src=\"https://hs-sandbox-pub.s3.us-east-1.amazonaws.com/blogs-draft/smith-trace.jpg\" alt=\"Langsmith trace view for AI engineers\" /> | <img src=\"https://hs-sandbox-pub.s3.us-east-1.amazonaws.com/blogs-draft/ls-trace.jpg\" alt=\"Label Studio trace view for domain experts\" /> |\n",
        "\n",
        "\n",
        "## Problem: Where my Agent failed?\n",
        "AI agent apps become hard to debug and improve as they grow more complex. Teams commonly struggle with:\n",
        "- Too much complexity: RAG pipelines, LLM calls, and multi-step conversations make it hard to know where problems occur\n",
        "- Getting stuck: After building prototypes, teams can't figure out what works, what breaks, or what to fix first\n",
        "- Missing expertise: Engineers often don't have the domain knowledge needed to properly evaluate their app's performance\n",
        "\n",
        "[The core activity proposed to address these issues is simply **\"Look at your data\"**.](https://hamel.dev/blog/posts/evals-faq/)\n",
        "\n",
        "However, who should look at the data is often unclear, yet this determines project success.\n",
        "\n",
        "## Our Solution: Give it to your Domain Expert\n",
        "\n",
        "We bridge this gap with a domain expert-centric UI that enables non-technical subject matter experts to systematically evaluate agent performance, complementing existing observability tools. This tutorial shows how to create an intuitive evaluation interface that transforms domain expertise into actionable improvements for your AI agents.\n",
        "\n",
        "\n",
        "## Setup: Build Your Agent Evaluation Pipeline\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "  <img src=\"https://hs-sandbox-pub.s3.us-east-1.amazonaws.com/blogs-draft/traces.jpg\" alt=\"Agent evaluation pipeline overview\" style=\"max-width: 600px; width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;\">\n",
        "</div>\n",
        "\n",
        "This tutorial demonstrates a complementary pipeline design for AI agent evaluation:\n",
        "\n",
        "**Step 1: Observability & Trace Collection**\n",
        "- Use observability tools (like Langsmith in this demo) for trace storage and collection\n",
        "- Engineer-centric interface for technical debugging and development iteration\n",
        "- Captures detailed execution traces, timing, and system-level metrics\n",
        "- Provides immediate feedback during development cycles\n",
        "\n",
        "**Step 2: Expert-Centric Evaluation in Label Studio**\n",
        "- Import traces from observability tools into Label Studio for human evaluation\n",
        "- Expert-centric, multi-user collaborative platform designed for domain specialists\n",
        "- Detailed UI for systematic assessment of each step in agentic workflows\n",
        "- Enables domain experts (SMEs) to identify and annotate failure modes\n",
        "- Supports team-based annotation workflows with inter-annotator agreement\n",
        "- Quality control features including annotation review and consensus building\n",
        "- Generates structured evaluation data for reports and LLM-as-a-Judge training\n",
        "\n",
        "**Pipeline Benefits:**\n",
        "- Engineers get technical observability while domain experts get intuitive evaluation tools\n",
        "- Traces flow seamlessly from development (observability) to evaluation (Label Studio)\n",
        "- Combines technical metrics with human domain expertise for comprehensive assessment\n",
        "- Note: While we use Langsmith as our demo observability tool, this approach works with any trace collection system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1bd599",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1bd599",
        "outputId": "fe9aa822-e8a4-46cc-fc16-dbb50b07eceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All environment variables loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Verify environment variables are loaded\n",
        "required_vars = ['LANGSMITH_API_KEY', 'LANGSMITH_PROJECT', 'LABEL_STUDIO_URL', 'LABEL_STUDIO_API_KEY', 'OPENAI_API_KEY']\n",
        "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"‚ö†Ô∏è  Missing environment variables: {', '.join(missing_vars)}\")\n",
        "    print(\"Please configure these in your .env file before proceeding.\")\n",
        "else:\n",
        "    print(\"‚úì All environment variables loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1415caf",
      "metadata": {
        "id": "e1415caf"
      },
      "source": [
        "## 2. Running the Agent\n",
        "\n",
        "We'll create a simple ReAct agent that can search academic papers on Arxiv.\n",
        "This agent simulates a production use case where we help researchers find\n",
        "relevant papers on specific topics.\n",
        "\n",
        "The agent uses:\n",
        "- **GPT-4o** as the reasoning engine\n",
        "- **Arxiv tool** to search academic papers\n",
        "- **Langsmith tracing** to capture all interactions automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ed35bb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ed35bb8",
        "outputId": "4ec90677-6151-448b-9ac2-3315b7e1106c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Agent initialized and ready\n"
          ]
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from uuid import uuid4\n",
        "\n",
        "# Initialize LLM and Tools\n",
        "llm = ChatOpenAI(model='gpt-4o')\n",
        "tools = load_tools([\"arxiv\"])\n",
        "\n",
        "# Pull the ReAct prompt template\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "# Create the agent\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        ")\n",
        "\n",
        "print(\"‚úì Agent initialized and ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00aa6a76",
      "metadata": {
        "id": "00aa6a76"
      },
      "source": [
        "### Running Agent Examples\n",
        "\n",
        "Let's simulate a user asking questions about latest research:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d03bc8a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d03bc8a4",
        "outputId": "b91d7748-f7ea-4d7d-d19c-5b5935db0899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer this question, I should search for the latest research articles on Visual Language Models on arXiv. This will provide me with the most recent developments and studies in this field.\n",
            "Action: arxiv\n",
            "Action Input: \"Visual Language Models\" latest\u001b[0m\u001b[36;1m\u001b[1;3mNo good Arxiv Result was found\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that the search query needs to be adjusted to find relevant and recent papers on Visual Language Models. I should try using related terms or recent developments in Visual Language Models to refine the search.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: \"Visual Grounding Language Models 2023\"\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2024-01-09\n",
            "Title: Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment\n",
            "Authors: Cong-Duy Nguyen, The-Anh Vu-Le, Thong Nguyen, Tho Quan, Luu Anh Tuan\n",
            "Summary: Language models have been supervised with both language-only objective and\n",
            "visual grounding in existing studies of visual-grounded language learning.\n",
            "However, due to differences in the distribution and scale of visual-grounded\n",
            "datasets and language corpora, the language model tends to mix up the context\n",
            "of the tokens that occurred in the grounded data with those that do not. As a\n",
            "result, during representation learning, there is a mismatch between the visual\n",
            "information and the contextual meaning of the sentence. To overcome this\n",
            "limitation, we propose GroundedBERT - a grounded language learning method that\n",
            "enhances the BERT representation with visually grounded information.\n",
            "GroundedBERT comprises two components: (i) the original BERT which captures the\n",
            "contextual representation of words learned from the language corpora, and (ii)\n",
            "a visual grounding module which captures visual information learned from\n",
            "visual-grounded datasets. Moreover, we employ Optimal Transport (OT),\n",
            "specifically its partial variant, to solve the fractional alignment problem\n",
            "between the two modalities. Our proposed method significantly outperforms the\n",
            "baseline language models on various language tasks of the GLUE and SQuAD\n",
            "datasets.\n",
            "\n",
            "Published: 2023-02-25\n",
            "Title: Visually-Augmented Language Modeling\n",
            "Authors: Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei\n",
            "Summary: Human language is grounded on multimodal knowledge including visual knowledge\n",
            "like colors, sizes, and shapes. However, current large-scale pre-trained\n",
            "language models rely on text-only self-supervised training with massive text\n",
            "data, which precludes them from utilizing relevant visual information when\n",
            "necessary. To address this, we propose a novel pre-training framework, named\n",
            "VaLM, to Visually-augment text tokens with retrieved relevant images for\n",
            "Language Modeling. Specifically, VaLM builds on a novel latent text-image\n",
            "alignment method via an image retrieval module to fetch corresponding images\n",
            "given a textual context. With the visually-augmented context, VaLM uses a\n",
            "visual knowledge fusion layer to enable multimodal grounded language modeling\n",
            "by attending to both text context and visual knowledge in images. We evaluate\n",
            "VaLM on various visual knowledge-intensive commonsense reasoning tasks, which\n",
            "require visual information to excel. The experimental results illustrate that\n",
            "VaLM outperforms all strong language-only and vision-language baselines with\n",
            "substantial gains in reasoning object commonsense including color, size, and\n",
            "shape. Our code is available at https://github.com/Victorwz/VaLM.\n",
            "\n",
            "Published: 2023-02-13\n",
            "Title: Like a bilingual baby: The advantage of visually grounding a bilingual language model\n",
            "Authors: Khai-Nguyen Nguyen, Zixin Tang, Ankur Mali, Alex Kelly\n",
            "Summary: Unlike most neural language models, humans learn language in a rich,\n",
            "multi-sensory and, often, multi-lingual environment. Current language models\n",
            "typically fail to fully capture the complexities of multilingual language use.\n",
            "We train an LSTM language model on images and captions in English and Spanish\n",
            "from MS-COCO-ES. We find that the visual grounding improves the model's\n",
            "understanding of semantic similarity both within and across languages and\n",
            "improves perplexity. However, we find no significant advantage of visual\n",
            "grounding for abstract words. Our results provide additional evidence of the\n",
            "advantages of visually grounded language models and point to the need for more\n",
            "naturalistic language data from multilingual speakers and multilingual datasets\n",
            "with perceptual grounding.\u001b[0m\u001b[32;1m\u001b[1;3mThe latest research on Visual Language Models explores various methods to enhance and ground language models with visual information. Here are some recent developments:\n",
            "\n",
            "1. **GroundedBERT**: This study proposes a method to enhance BERT representations with visually grounded information. It combines BERT's contextual word representations with a visual grounding module that captures visual data. The approach uses Optimal Transport for partial alignment between these modalities and shows improved performance on language tasks compared to baseline models.\n",
            "\n",
            "2. **VaLM (Visually-Augmented Language Modeling)**: This framework augments text tokens with images retrieved based on the textual context to enrich language models with visual knowledge such as colors, sizes, and shapes. The visually augmented context is processed using a visual knowledge fusion layer. VaLM has shown substantial improvements in visual commonsense reasoning tasks over traditional language-only models and vision-language baselines.\n",
            "\n",
            "3. **Bilingual Language Modeling with Visual Grounding**: This research trains an LSTM model using bilingual (English and Spanish) datasets with visual grounding. It finds that visual grounding enhances the model's semantic understanding across languages and improves perplexity but does not significantly benefit the comprehension of abstract terms. This highlights the potential of visually grounded models but also their limitations with abstract language.\n",
            "\n",
            "These studies indicate ongoing efforts to integrate visual information into language models for improved performance in multimodal and multilingual contexts.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mFinal Answer: The latest research in Visual Language Models includes several innovative approaches:\n",
            "\n",
            "1. **GroundedBERT**: This method enhances BERT with visually grounded information to improve the contextual understanding of text by addressing misalignments between text and visual data using Optimal Transport. It has shown superior performance on language tasks like GLUE and SQuAD.\n",
            "\n",
            "2. **VaLM (Visually-Augmented Language Modeling)**: VaLM incorporates visual information by augmenting text with context-relevant images, using a visual knowledge fusion layer. It has demonstrated improved performance in visual commonsense reasoning tasks, surpassing both language-only and vision-language models.\n",
            "\n",
            "3. **Bilingual Language Modeling with Visual Grounding**: This research employs visual grounding in bilingual (English-Spanish) language models to enhance semantic similarity understanding across languages, although it shows less impact on abstract concepts. It underscores the benefits and limitations of using visual data in multilingual contexts. \n",
            "\n",
            "These studies illustrate the significant impact of integrating visual information into language models, optimizing them for varied and complex tasks by providing them with additional contextual data from images.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "‚úì Run 1 completed. Run ID: 2401fa2a-9e96-4bbb-9441-f48375729d2a\n",
            "Question: What is the latest research on Visual Language Models?\n",
            "Response: The latest research in Visual Language Models includes several innovative approaches:\n",
            "\n",
            "1. **GroundedBERT**: This method enhances BERT with visually grounded information to improve the contextual under...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo find papers about reinforcement learning in robotics from the last year, I should search the arXiv database with a query that includes \"reinforcement learning\", \"robotics\", and filters the results to the last year. This should provide relevant and recent academic papers on the topic.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: reinforcement learning robotics submittedDate:[2022-10-01 TO 2023-10-01]\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2022-10-13\n",
            "Title: A Concise Introduction to Reinforcement Learning in Robotics\n",
            "Authors: Akash Nagaraj, Mukund Sood, Bhagya M Patil\n",
            "Summary: One of the biggest hurdles robotics faces is the facet of sophisticated and\n",
            "hard-to-engineer behaviors. Reinforcement learning offers a set of tools, and a\n",
            "framework to address this problem. In parallel, the misgivings of robotics\n",
            "offer a solid testing ground and evaluation metric for advancements in\n",
            "reinforcement learning. The two disciplines go hand-in-hand, much like the\n",
            "fields of Mathematics and Physics. By means of this survey paper, we aim to\n",
            "invigorate links between the research communities of the two disciplines by\n",
            "focusing on the work done in reinforcement learning for locomotive and control\n",
            "aspects of robotics. Additionally, we aim to highlight not only the notable\n",
            "successes but also the key challenges of the application of Reinforcement\n",
            "Learning in Robotics. This paper aims to serve as a reference guide for\n",
            "researchers in reinforcement learning applied to the field of robotics. The\n",
            "literature survey is at a fairly introductory level, aimed at aspiring\n",
            "researchers. Appropriately, we have covered the most essential concepts\n",
            "required for research in the field of reinforcement learning, with robotics in\n",
            "mind. Through a thorough analysis of this problem, we are able to manifest how\n",
            "reinforcement learning could be applied profitably, and also focus on\n",
            "open-ended questions, as well as the potential for future research.\n",
            "\n",
            "Published: 2022-09-07\n",
            "Title: A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform\n",
            "Authors: Z. Jiang, G. Song\n",
            "Summary: With the development of industry, drones are appearing in various field. In\n",
            "recent years, deep reinforcement learning has made impressive gains in games,\n",
            "and we are committed to applying deep reinforcement learning algorithms to the\n",
            "field of robotics, moving reinforcement learning algorithms from game scenarios\n",
            "to real-world application scenarios. We are inspired by the LunarLander of\n",
            "OpenAI Gym, we decided to make a bold attempt in the field of reinforcement\n",
            "learning to control drones. At present, there is still a lack of work applying\n",
            "reinforcement learning algorithms to robot control, the physical simulation\n",
            "platform related to robot control is only suitable for the verification of\n",
            "classical algorithms, and is not suitable for accessing reinforcement learning\n",
            "algorithms for the training. In this paper, we will face this problem, bridging\n",
            "the gap between physical simulation platforms and intelligent agent, connecting\n",
            "intelligent agents to a physical simulation platform, allowing agents to learn\n",
            "and complete drone flight tasks in a simulator that approximates the real\n",
            "world. We proposed a reinforcement learning framework based on Gazebo that is a\n",
            "kind of physical simulation platform (ROS-RL), and used three continuous action\n",
            "space reinforcement learning algorithms in the framework to dealing with the\n",
            "problem of autonomous landing of drones. Experiments show the effectiveness of\n",
            "the algorithm, the task of autonomous landing of drones based on reinforcement\n",
            "learning achieved full success.\n",
            "\n",
            "Published: 2022-05-20\n",
            "Title: Adversarial joint attacks on legged robots\n",
            "Authors: Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto\n",
            "Summary: We address adversarial attacks on the actuators at the joints of legged\n",
            "robots trained by deep reinforcement learning. The vulnerability to the joint\n",
            "attacks can significantly impact the safety and robustness of legged robots. In\n",
            "this study, we demonstrate that the adversarial perturbations to the torque\n",
            "control signals of the actuators can significantly reduce the rewards and cause\n",
            "walking instability in robots. To find the adversarial torque perturbations, we\n",
            "develop black-box adversarial attacks, where, the adversary cannot access the\n",
            "neural networks trained by deep reinforcement learning. The black box attack\n",
            "can be applied to legged robots regardless of the architecture and algorithms\n",
            "of deep r\u001b[0m\u001b[32;1m\u001b[1;3mThe search returned relevant papers on reinforcement learning in robotics, published within the last year. Here are the details of the papers identified:\n",
            "\n",
            "1. **A Concise Introduction to Reinforcement Learning in Robotics**\n",
            "   - **Published:** 2022-10-13\n",
            "   - **Authors:** Akash Nagaraj, Mukund Sood, Bhagya M Patil\n",
            "   - **Summary:** This paper provides a survey of reinforcement learning in robotics, discussing its potential to address complex behaviors in robotics and evaluating its successes and challenges. It's aimed at aspiring researchers and provides essential concepts needed for research in this field.\n",
            "\n",
            "2. **A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform**\n",
            "   - **Published:** 2022-09-07\n",
            "   - **Authors:** Z. Jiang, G. Song\n",
            "   - **Summary:** This paper explores the application of deep reinforcement learning for the autonomous landing of drones. The authors propose a framework using a physical simulation platform and demonstrate the effectiveness of continuous action space reinforcement learning algorithms for this task.\n",
            "\n",
            "3. **Adversarial Joint Attacks on Legged Robots**\n",
            "   - **Published:** 2022-05-20\n",
            "   - **Authors:** Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto\n",
            "   - **Summary:** This study addresses adversarial attacks on the actuators of legged robots trained by deep reinforcement learning. The paper explores black-box adversarial attacks to demonstrate the vulnerability of robots and the impact on their stability and safety.\n",
            "\n",
            "These papers provide a broad insight into recent research and developments in the use of reinforcement learning in robotics.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mThe search returned relevant papers on reinforcement learning in robotics, published within the last year. Here are the details of the papers identified:\n",
            "\n",
            "1. **A Concise Introduction to Reinforcement Learning in Robotics**\n",
            "   - **Published:** 2022-10-13\n",
            "   - **Authors:** Akash Nagaraj, Mukund Sood, Bhagya M Patil\n",
            "   - **Summary:** This paper provides a survey of reinforcement learning in robotics, discussing its potential to address complex behaviors in robotics and evaluating its successes and challenges. It's aimed at aspiring researchers and provides essential concepts needed for research in this field.\n",
            "\n",
            "2. **A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform**\n",
            "   - **Published:** 2022-09-07\n",
            "   - **Authors:** Z. Jiang, G. Song\n",
            "   - **Summary:** This paper explores the application of deep reinforcement learning for the autonomous landing of drones. The authors propose a framework using a physical simulation platform and demonstrate the effectiveness of continuous action space reinforcement learning algorithms for this task.\n",
            "\n",
            "3. **Adversarial Joint Attacks on Legged Robots**\n",
            "   - **Published:** 2022-05-20\n",
            "   - **Authors:** Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto\n",
            "   - **Summary:** This study addresses adversarial attacks on the actuators of legged robots trained by deep reinforcement learning. The paper explores black-box adversarial attacks to demonstrate the vulnerability of robots and the impact on their stability and safety.\n",
            "\n",
            "These papers provide a broad insight into recent research and developments in the use of reinforcement learning in robotics.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mHere are the recent papers on reinforcement learning in robotics from the last year:\n",
            "\n",
            "1. **A Concise Introduction to Reinforcement Learning in Robotics**\n",
            "   - **Published:** 2022-10-13\n",
            "   - **Authors:** Akash Nagaraj, Mukund Sood, Bhagya M Patil\n",
            "   - **Summary:** This paper surveys the field of reinforcement learning in robotics, focusing on its application in addressing sophisticated behaviors in robotics and evaluating both its successes and challenges. It serves as a reference for aspiring researchers by introducing essential concepts needed for research in this area.\n",
            "\n",
            "2. **A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform**\n",
            "   - **Published:** 2022-09-07\n",
            "   - **Authors:** Z. Jiang, G. Song\n",
            "   - **Summary:** The paper discusses using deep reinforcement learning for the autonomous landing of drones. It presents a framework utilizing physical simulation platforms, demonstrating the effectiveness of continuous action space reinforcement learning algorithms in achieving successful drone landings.\n",
            "\n",
            "These papers highlight ongoing research and developments in the application of reinforcement learning techniques to robotics.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mI found some recent papers related to reinforcement learning in robotics. Here are the details:\n",
            "\n",
            "1. **A Concise Introduction to Reinforcement Learning in Robotics**\n",
            "   - **Published:** 2022-10-13\n",
            "   - **Authors:** Akash Nagaraj, Mukund Sood, Bhagya M Patil\n",
            "   - **Summary:** This paper surveys the field of reinforcement learning in robotics, highlighting its potential to address complex behaviors and challenges in robotics. It serves as an introductory reference for researchers interested in reinforcement learning applications in robotics.\n",
            "\n",
            "2. **A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform**\n",
            "   - **Published:** 2022-09-07\n",
            "   - **Authors:** Z. Jiang, G. Song\n",
            "   - **Summary:** This study explores the use of deep reinforcement learning for the autonomous landing of drones, demonstrating the effectiveness of continuous action space reinforcement learning algorithms when integrated with a physical simulation platform.\n",
            "\n",
            "These papers offer insights into the current research landscape of reinforcement learning in robotics from the last year.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "\n",
            "Final Answer: Here are some recent papers on reinforcement learning in robotics from the last year:\n",
            "\n",
            "1. **A Concise Introduction to Reinforcement Learning in Robotics**\n",
            "   - **Published:** 2022-10-13\n",
            "   - **Authors:** Akash Nagaraj, Mukund Sood, Bhagya M Patil\n",
            "   - **Summary:** This paper provides a survey of reinforcement learning in robotics, focusing on its potential to address complex behaviors and evaluating both successes and challenges. It serves as a reference guide for aspiring researchers.\n",
            "\n",
            "2. **A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform**\n",
            "   - **Published:** 2022-09-07\n",
            "   - **Authors:** Z. Jiang, G. Song\n",
            "   - **Summary:** The study applies deep reinforcement learning to the autonomous landing of drones using a physical simulation platform, demonstrating the effectiveness of continuous action space reinforcement learning algorithms.\n",
            "\n",
            "These papers highlight ongoing research and developments in applying reinforcement learning techniques to robotics.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "‚úì Run 2 completed. Run ID: e92abdf7-c2c0-4acf-b26f-f0cf18745595\n",
            "Question: Find papers about reinforcement learning in robotics from the last year\n",
            "Response: Here are some recent papers on reinforcement learning in robotics from the last year:\n",
            "\n",
            "1. **A Concise Introduction to Reinforcement Learning in Robotics**\n",
            "   - **Published:** 2022-10-13\n",
            "   - **Authors...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer this question, I will search for recent papers on transformer architectures on arXiv to get the latest innovations and developments. \n",
            "Action: arxiv\n",
            "Action Input: \"latest innovations transformer architectures 2023\"\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2023-06-02\n",
            "Title: Transforming ECG Diagnosis:An In-depth Review of Transformer-based DeepLearning Models in Cardiovascular Disease Detection\n",
            "Authors: Zibin Zhao\n",
            "Summary: The emergence of deep learning has significantly enhanced the analysis of\n",
            "electrocardiograms (ECGs), a non-invasive method that is essential for\n",
            "assessing heart health. Despite the complexity of ECG interpretation, advanced\n",
            "deep learning models outperform traditional methods. However, the increasing\n",
            "complexity of ECG data and the need for real-time and accurate diagnosis\n",
            "necessitate exploring more robust architectures, such as transformers. Here, we\n",
            "present an in-depth review of transformer architectures that are applied to ECG\n",
            "classification. Originally developed for natural language processing, these\n",
            "models capture complex temporal relationships in ECG signals that other models\n",
            "might overlook. We conducted an extensive search of the latest\n",
            "transformer-based models and summarize them to discuss the advances and\n",
            "challenges in their application and suggest potential future improvements. This\n",
            "review serves as a valuable resource for researchers and practitioners and aims\n",
            "to shed light on this innovative application in ECG interpretation.\n",
            "\n",
            "Published: 2023-09-13\n",
            "Title: Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts\n",
            "Authors: Rishov Sarkar, Hanxue Liang, Zhiwen Fan, Zhangyang Wang, Cong Hao\n",
            "Summary: Computer vision researchers are embracing two promising paradigms: Vision\n",
            "Transformers (ViTs) and Multi-task Learning (MTL), which both show great\n",
            "performance but are computation-intensive, given the quadratic complexity of\n",
            "self-attention in ViT and the need to activate an entire large MTL model for\n",
            "one task. M$^3$ViT is the latest multi-task ViT model that introduces\n",
            "mixture-of-experts (MoE), where only a small portion of subnetworks (\"experts\")\n",
            "are sparsely and dynamically activated based on the current task. M$^3$ViT\n",
            "achieves better accuracy and over 80% computation reduction but leaves\n",
            "challenges for efficient deployment on FPGA.\n",
            "  Our work, dubbed Edge-MoE, solves the challenges to introduce the first\n",
            "end-to-end FPGA accelerator for multi-task ViT with a collection of\n",
            "architectural innovations, including (1) a novel reordering mechanism for\n",
            "self-attention, which requires only constant bandwidth regardless of the target\n",
            "parallelism; (2) a fast single-pass softmax approximation; (3) an accurate and\n",
            "low-cost GELU approximation; (4) a unified and flexible computing unit that is\n",
            "shared by almost all computational layers to maximally reduce resource usage;\n",
            "and (5) uniquely for M$^3$ViT, a novel patch reordering method to eliminate\n",
            "memory access overhead. Edge-MoE achieves 2.24x and 4.90x better energy\n",
            "efficiency comparing with GPU and CPU, respectively. A real-time video\n",
            "demonstration is available online, along with our open-source code written\n",
            "using High-Level Synthesis.\n",
            "\n",
            "Published: 2025-08-04\n",
            "Title: YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges\n",
            "Authors: Manikanta Kotthapalli, Deepika Ravipati, Reshma Bhatia\n",
            "Summary: Over the past decade, object detection has advanced significantly, with the\n",
            "YOLO (You Only Look Once) family of models transforming the landscape of\n",
            "real-time vision applications through unified, end-to-end detection frameworks.\n",
            "From YOLOv1's pioneering regression-based detection to the latest YOLOv9, each\n",
            "version has systematically enhanced the balance between speed, accuracy, and\n",
            "deployment efficiency through continuous architectural and algorithmic\n",
            "advancements.. Beyond core object detection, modern YOLO architectures have\n",
            "expanded to support tasks such as instance segmentation, pose estimation,\n",
            "object tracking, and domain-specific applications including medical imaging and\n",
            "industrial automation. This paper offers a comprehensive review of the YOLO\n",
            "family, highlighting architectural innovations, performance benchmarks,\n",
            "ext\u001b[0m\u001b[32;1m\u001b[1;3mWhile not directly applicable, the provided paper on YOLO highlights innovations in real-time object detection rather than transformers. It is clear I should focus on details specifically concerning transformers, especially in new roles or applications.\n",
            "\n",
            "Here are some significant innovations from the retrieved papers on transformer architectures:\n",
            "\n",
            "1. **Transformers in ECG Diagnosis**: Transformers, originally developed for natural language processing, have found new applications in ECG data analysis due to their ability to capture complex temporal relationships within biological signals. These models surpass traditional methods in real-time and accurate diagnostics, suggesting new potential for improving cardiovascular disease detection.\n",
            "\n",
            "2. **Edge-MoE Vision Transformers**: Innovations like Edge-MoE reflect an effort to enhance the computational efficiency of Vision Transformers. This architecture combines multi-task learning with sparse activation of subnetworks, yielding performance gains while necessitating architectural innovations for hardware like FPGA. The methods utilized here, like reordering and approximations for softmax, illustrate ongoing efforts to balance performance with resource efficiency in transformers.\n",
            "\n",
            "These examples demonstrate transformers stretching beyond natural language to domains like medical data analysis, with adaptations in hardware for efficiency. This expansion into diverse tasks showcases their adaptability and the continuous evolution of their architectures. \n",
            "\n",
            "Final Answer: Recent innovations in transformer architectures include their application to ECG diagnostics for better capturing temporal relationships, and advancements like Edge-MoE, which enhance computational efficiency in Vision Transformers through novel FPGA architectural techniques.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "‚úì Run 3 completed. Run ID: ae390f69-6922-43f1-a98a-36c5da9f1af8\n",
            "Question: Explain the latest innovations in transformer architectures based on recent papers\n",
            "Response: Recent innovations in transformer architectures include their application to ECG diagnostics for better capturing temporal relationships, and advancements like Edge-MoE, which enhance computational ef...\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"What is the latest research on Visual Language Models?\",\n",
        "    \"Find papers about reinforcement learning in robotics from the last year\",\n",
        "    \"Explain the latest innovations in transformer architectures based on recent papers\"\n",
        "]\n",
        "\n",
        "for i, question in enumerate(questions, 1):\n",
        "    config = RunnableConfig(run_id=str(uuid4()))\n",
        "    result = agent_executor.invoke({\"input\": question}, config=config)\n",
        "\n",
        "    print(f\"\\n‚úì Run {i} completed. Run ID: {config.get('run_id')}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Response: {result['output'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e419f6f",
      "metadata": {
        "id": "6e419f6f"
      },
      "source": [
        "## 3. Collecting Traces\n",
        "\n",
        "Now that we've generated several agent interactions, we'll:\n",
        "1. Connect to Langsmith and [query Agent traces](https://docs.langchain.com/langsmith/export-traces)\n",
        "2. Filter for relevant run types (AgentExecutor, LLM, Tool, Retriever)\n",
        "3. Convert traces to [Label Studio format with predictions](https://docs.humansignal.com/guide/predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ab8504",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25ab8504",
        "outputId": "65cb7f26-8755-4485-919c-95e89f8c28dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Collected 24 traces from Langsmith\n",
            "  Total runs: 149\n"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "import json\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Initialize Langsmith client\n",
        "client = Client()\n",
        "\n",
        "# Define filter to get relevant run types\n",
        "# We want to capture the complete agent execution flow\n",
        "filter_string = 'or(and(eq(name, \"AgentExecutor\"), eq(run_type, \"chain\")), eq(run_type, \"llm\"), eq(run_type, \"tool\"), eq(run_type, \"retriever\"))'\n",
        "\n",
        "# Fetch all runs from the project\n",
        "runs = client.list_runs(\n",
        "    project_name=os.getenv('LANGSMITH_PROJECT'),\n",
        "    filter=filter_string\n",
        ")\n",
        "\n",
        "# Sort runs by start_time for chronological processing\n",
        "runs = sorted(runs, key=lambda run: run.start_time)\n",
        "\n",
        "# Group runs by trace_id\n",
        "traces = {}\n",
        "for run in runs:\n",
        "    if run.trace_id not in traces:\n",
        "        traces[run.trace_id] = []\n",
        "    traces[run.trace_id].append(run.dict())\n",
        "\n",
        "print(f\"‚úì Collected {len(traces)} traces from Langsmith\")\n",
        "print(f\"  Total runs: {len(runs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f64556",
      "metadata": {
        "id": "a8f64556"
      },
      "source": [
        "### 4. Handler Functions for Different Run Types\n",
        "\n",
        "These functions convert different types of Langsmith runs into Label Studio's\n",
        "chat message format. Each run type has specific data structures that need to be\n",
        "transformed appropriately."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent requests LLM provider\n",
        "AGENT_LLM_CALL = 'agent_llm_call'\n",
        "# Agent receives response from LLM provider\n",
        "AGENT_LLM_RESPONSE = 'agent_llm_response'\n",
        "# Agent calls tool (e.g. API call)\n",
        "AGENT_TOOL_CALL = 'agent_tool_call'\n",
        "# Agent calls retriever\n",
        "AGENT_RETRIEVER_CALL = 'agent_retriever_call'"
      ],
      "metadata": {
        "id": "0R4puASbaxr_"
      },
      "id": "0R4puASbaxr_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b845e3",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56b845e3",
        "outputId": "bcebe7d9-0533-490c-a29a-c1b826cb3d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Handler functions defined\n"
          ]
        }
      ],
      "source": [
        "def handle_llm(run, messages):\n",
        "    \"\"\"\n",
        "    Handler for LLM runs (run_type: \"llm\").\n",
        "    Extracts user messages from inputs and assistant responses from outputs.\n",
        "    \"\"\"\n",
        "    # Extract user message from inputs if present\n",
        "    if 'inputs' in run and 'messages' in run['inputs']:\n",
        "        messages_input = run['inputs']['messages']\n",
        "        if messages_input and len(messages_input) > 0:\n",
        "            message_array = messages_input[0]\n",
        "            if message_array and len(message_array) > 0:\n",
        "                message = message_array[0]\n",
        "                if 'kwargs' in message and 'content' in message['kwargs']:\n",
        "                    messages.append({\n",
        "                        \"content\": message['kwargs']['content'],\n",
        "                        \"role\": \"user\",\n",
        "                        \"_hs_meta\": {\n",
        "                            \"role\": AGENT_LLM_CALL\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "    # Extract assistant response from outputs\n",
        "    if 'outputs' in run and 'generations' in run['outputs']:\n",
        "        generations = run['outputs']['generations']\n",
        "        if generations and len(generations) > 0:\n",
        "            generation = generations[0]\n",
        "            if 'text' in generation:\n",
        "                messages.append({\n",
        "                    \"content\": generation['text'],\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"_hs_meta\": {\n",
        "                        \"role\": AGENT_LLM_RESPONSE\n",
        "                    }\n",
        "                })\n",
        "            elif 'message' in generation:\n",
        "                message = generation['message']\n",
        "                if 'kwargs' in message and 'content' in message['kwargs']:\n",
        "                    messages.append({\n",
        "                        \"content\": message['kwargs']['content'],\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"_hs_meta\": {\n",
        "                          \"role\": AGENT_LLM_RESPONSE\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "\n",
        "def handle_tool(run, messages):\n",
        "    \"\"\"\n",
        "    Handler for Tool runs (run_type: \"tool\").\n",
        "    Creates assistant message with tool call and tool response message.\n",
        "    \"\"\"\n",
        "    # Tool metadata\n",
        "    tool_name = run.get('name', 'unknown_tool')\n",
        "    tool_call_id = str(uuid.uuid4())[:8]\n",
        "\n",
        "    # Extract tool input\n",
        "    tool_input = \"\"\n",
        "    if 'inputs' in run and 'input' in run['inputs']:\n",
        "        tool_input = run['inputs']['input']\n",
        "\n",
        "    # Add assistant message with tool call\n",
        "    messages.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"tool_calls\": [\n",
        "            {\n",
        "                \"id\": tool_call_id,\n",
        "                \"type\": \"custom\",\n",
        "                \"custom\": {\n",
        "                    \"name\": tool_name,\n",
        "                    \"input\": tool_input\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        \"_hs_meta\": {\n",
        "            \"role\": AGENT_TOOL_CALL\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # Add tool response message\n",
        "    if 'outputs' in run and 'output' in run['outputs']:\n",
        "        messages.append({\n",
        "            \"content\": run['outputs']['output'],\n",
        "            \"role\": \"tool\",\n",
        "            \"tool_call_id\": tool_call_id\n",
        "        })\n",
        "\n",
        "\n",
        "def handle_retriever(run, messages):\n",
        "    \"\"\"\n",
        "    Handler for Retriever runs (run_type: \"retriever\").\n",
        "    Extracts retrieval results and formats them as tool messages.\n",
        "    \"\"\"\n",
        "    if 'outputs' in run:\n",
        "        output_content = str(run['outputs'])\n",
        "        messages.append({\n",
        "            \"content\": f\"Retriever result: {output_content}\",\n",
        "            \"role\": \"tool\",\n",
        "            \"_hs_meta\": {\n",
        "                \"role\": AGENT_RETRIEVER_CALL\n",
        "            }\n",
        "        })\n",
        "\n",
        "print(\"‚úì Handler functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "894df053",
      "metadata": {
        "id": "894df053"
      },
      "source": [
        "### 5. Convert Traces to Label Studio Format\n",
        "\n",
        "This function transforms Langsmith traces into Label Studio's task format.\n",
        "The `mode=\"predictions\"` parameter creates pre-annotations that annotators\n",
        "can review and modify, speeding up the evaluation process.\n",
        "\n",
        "### Key Features:\n",
        "- Preserves chronological order of agent interactions\n",
        "- Maintains timestamps for temporal analysis\n",
        "- Structures data to match Label Studio's Chat interface\n",
        "- Creates predictions for each message (except the initial user query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da5fca02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da5fca02",
        "outputId": "13330ce7-0162-4742-bd05-a3aa8583848b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Converted 24 traces to Label Studio format\n"
          ]
        }
      ],
      "source": [
        "def convert_trace_to_messages(trace_id, trace_runs, mode=\"input\", from_name=\"chat\"):\n",
        "    \"\"\"\n",
        "    Converts a trace (list of runs) to Label Studio format.\n",
        "\n",
        "    Args:\n",
        "        trace_id: Unique identifier for the trace\n",
        "        trace_runs: List of run dictionaries from a single trace\n",
        "        mode: Either \"input\" or \"predictions\"\n",
        "              - \"input\": Returns simple message list\n",
        "              - \"predictions\": Returns Label Studio task format with predictions\n",
        "        from_name: The from_name field for Label Studio predictions\n",
        "\n",
        "    Returns:\n",
        "        Dict with trace_id and either messages or Label Studio task structure\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    agent_executor_run = None\n",
        "\n",
        "    # Sort runs chronologically\n",
        "    sorted_runs = sorted(trace_runs, key=lambda r: r.get('start_time', ''))\n",
        "\n",
        "    # Find AgentExecutor run (the main chain)\n",
        "    trace_id_str = None\n",
        "    for run in sorted_runs:\n",
        "        if not trace_id_str and 'trace_id' in run:\n",
        "            trace_id_str = str(run['trace_id'])\n",
        "        if run.get('name') == 'AgentExecutor' and run.get('run_type') == 'chain':\n",
        "            agent_executor_run = run\n",
        "            break\n",
        "\n",
        "    # Extract initial user message from AgentExecutor\n",
        "    first_user_timestamp = None\n",
        "    if agent_executor_run:\n",
        "        if 'inputs' in agent_executor_run and 'input' in agent_executor_run['inputs']:\n",
        "            first_user_timestamp = agent_executor_run.get('start_time')\n",
        "            messages.append({\n",
        "                \"content\": agent_executor_run['inputs']['input'],\n",
        "                \"role\": \"user\",\n",
        "                \"_timestamp\": first_user_timestamp\n",
        "            })\n",
        "\n",
        "    # Process intermediate runs (llm, tool, retriever)\n",
        "    for run in sorted_runs:\n",
        "        run_type = run.get('run_type')\n",
        "        name = run.get('name')\n",
        "\n",
        "        # Skip the AgentExecutor itself\n",
        "        if name == 'AgentExecutor' and run_type == 'chain':\n",
        "            continue\n",
        "\n",
        "        # Track messages before processing\n",
        "        messages_before = len(messages)\n",
        "\n",
        "        # Apply appropriate handler\n",
        "        if run_type == 'llm':\n",
        "            handle_llm(run, messages)\n",
        "        elif run_type == 'tool':\n",
        "            handle_tool(run, messages)\n",
        "        elif run_type == 'retriever':\n",
        "            handle_retriever(run, messages)\n",
        "\n",
        "        # Attach timestamp to new messages\n",
        "        messages_after = len(messages)\n",
        "        run_timestamp = run.get('start_time')\n",
        "        for i in range(messages_before, messages_after):\n",
        "            if '_timestamp' not in messages[i]:\n",
        "                messages[i]['_timestamp'] = run_timestamp\n",
        "\n",
        "    # Add final assistant message from AgentExecutor output\n",
        "    if agent_executor_run:\n",
        "        if agent_executor_run.get('outputs') and 'output' in agent_executor_run['outputs']:\n",
        "            final_output = agent_executor_run['outputs']['output']\n",
        "            # Only add if different from last message\n",
        "            if not messages or messages[-1]['content'] != final_output:\n",
        "                messages.append({\n",
        "                    \"content\": final_output,\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"_timestamp\": agent_executor_run.get('end_time') or agent_executor_run.get('start_time')\n",
        "                })\n",
        "\n",
        "    # Return format depends on mode\n",
        "    if mode == \"input\":\n",
        "        # Clean format without timestamps\n",
        "        clean_messages = []\n",
        "        for msg in messages:\n",
        "            clean_msg = {k: v for k, v in msg.items() if k != '_timestamp'}\n",
        "            clean_messages.append(clean_msg)\n",
        "        return {\"trace_id\": str(trace_id_str), \"messages\": clean_messages}\n",
        "\n",
        "    # Mode is \"predictions\" - convert to Label Studio format\n",
        "    if mode == \"predictions\":\n",
        "        # Separate first user message from remaining messages\n",
        "        first_user_message = None\n",
        "        remaining_messages = []\n",
        "\n",
        "        for msg in messages:\n",
        "            if msg.get('role') == 'user' and first_user_message is None:\n",
        "                first_user_message = {k: v for k, v in msg.items() if k != '_timestamp'}\n",
        "            elif first_user_message is not None:\n",
        "                remaining_messages.append(msg)\n",
        "\n",
        "        # Convert remaining messages to predictions\n",
        "        predictions_result = []\n",
        "\n",
        "        for msg in remaining_messages:\n",
        "            # Generate unique ID for prediction\n",
        "            prediction_id = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
        "\n",
        "            # Parse timestamp\n",
        "            created_at = None\n",
        "            timestamp_value = msg.get('_timestamp')\n",
        "\n",
        "            if timestamp_value:\n",
        "                try:\n",
        "                    if isinstance(timestamp_value, datetime):\n",
        "                        dt = timestamp_value\n",
        "                    elif isinstance(timestamp_value, str):\n",
        "                        if 'T' in timestamp_value:\n",
        "                            dt = datetime.fromisoformat(timestamp_value.replace('Z', '+00:00'))\n",
        "                        else:\n",
        "                            dt = datetime.fromisoformat(timestamp_value)\n",
        "                    else:\n",
        "                        dt = None\n",
        "\n",
        "                    if dt:\n",
        "                        created_at = int(dt.timestamp() * 1000)\n",
        "                except Exception as e:\n",
        "                    print(f'Warning: Error parsing timestamp: {timestamp_value} - {e}')\n",
        "\n",
        "            # Fallback to current timestamp\n",
        "            if created_at is None:\n",
        "                created_at = int(datetime.now().timestamp() * 1000)\n",
        "\n",
        "            # Build prediction item\n",
        "            prediction_item = {\n",
        "                \"id\": prediction_id,\n",
        "                \"type\": \"chatmessage\",\n",
        "                \"value\": {\n",
        "                    \"chatmessage\": {\n",
        "                        \"role\": msg.get('role', 'assistant'),\n",
        "                        \"content\": msg.get('content', ''),\n",
        "                        \"createdAt\": created_at\n",
        "                    }\n",
        "                },\n",
        "                \"origin\": \"manual\",\n",
        "                \"to_name\": \"chat\",\n",
        "                \"from_name\": from_name\n",
        "            }\n",
        "\n",
        "            # Include tool_calls if present\n",
        "            if 'tool_calls' in msg:\n",
        "                prediction_item['value']['chatmessage']['tool_calls'] = msg['tool_calls']\n",
        "                # prediction_item['value']['chatmessage'].pop('content', None)\n",
        "\n",
        "            # Include tool_call_id if present\n",
        "            if 'tool_call_id' in msg:\n",
        "                prediction_item['value']['chatmessage']['tool_call_id'] = msg['tool_call_id']\n",
        "\n",
        "            # Include _hs_meta if present\n",
        "            if '_hs_meta' in msg:\n",
        "                prediction_item['value']['chatmessage']['_hs_meta'] = msg['_hs_meta']\n",
        "\n",
        "            predictions_result.append(prediction_item)\n",
        "\n",
        "        # Build final task structure\n",
        "        task_data = {\n",
        "            \"data\": {\n",
        "                \"trace_id\": str(trace_id_str),\n",
        "                \"messages\": [first_user_message] if first_user_message else []\n",
        "            },\n",
        "            \"predictions\": [{\n",
        "                \"result\": predictions_result\n",
        "            }]\n",
        "        }\n",
        "\n",
        "        return task_data\n",
        "\n",
        "    return {\"trace_id\": str(trace_id_str), \"messages\": messages}\n",
        "\n",
        "\n",
        "# Convert all traces to Label Studio format\n",
        "tasks = []\n",
        "for trace_id, trace_data in traces.items():\n",
        "    task = convert_trace_to_messages(trace_id, trace_data, mode=\"predictions\")\n",
        "    tasks.append(task)\n",
        "\n",
        "print(f\"‚úì Converted {len(tasks)} traces to Label Studio format\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79bdd916",
      "metadata": {
        "id": "79bdd916"
      },
      "source": [
        "## 6. Evaluate Traces\n",
        "\n",
        "To run evaluation on the collected agent traces, use [Label Studio XML configuration](https://docs.humansignal.com/guide/setup) to provide a specialized UI with:\n",
        "\n",
        "- **Chat Display**: Shows the full conversation with all agent interactions\n",
        "- **Message-Level Annotation**: Click any message to annotate specific errors\n",
        "- **Role-Specific Error Categories**:\n",
        "  - User errors (request issues)\n",
        "  - Assistant errors (hallucination, bias, format, etc.)\n",
        "  - Tool errors (tool malfunction or incorrect output)\n",
        "- **Overall Assessment**: Rate the entire conversation\n",
        "- **Notes Fields**: Capture detailed feedback from annotators\n",
        "\n",
        "This configuration enables Subject Matter Experts (SMEs) to perform detailed\n",
        "failure mode analysis on each step of the agent's reasoning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8fedb59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8fedb59",
        "outputId": "705a05b0-94e4-48d9-a152-1e15726b91a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Label configuration defined\n"
          ]
        }
      ],
      "source": [
        "label_config = \"\"\"<View>\n",
        "\t<Style>\n",
        "    .htx-chat{flex-grow:1}\n",
        "    .htx-chat-sidepanel{flex:300px 0 0;display:flex;flex-direction:column;border-left:2px solid #ccc;padding-left:16px}\n",
        "    .htx-chat-container{display:flex;flex-direction:column;flex-grow:1}\n",
        "    .htx-overall-assessment{margin-top:16px;padding:16px;border:1px solid #e0e0e0;border-radius:8px;background:#fafafa}\n",
        "  </Style>\n",
        "\t<View style=\"display:flex;width:100%;gap:1em\">\n",
        "\t\t<View className=\"htx-chat-container\">\n",
        "\t\t\t<Chat name=\"chat\"\n",
        "\t              value=\"$messages\"\n",
        "\t              llm=\"openai/gpt-5-mini\"\n",
        "\t              editable=\"true\"\n",
        "\t              messageroles=\"user,assistant,tool\"\n",
        "\t              />\n",
        "\t\t\t<!-- Evaluate the whole conversation -->\n",
        "\t\t\t<View className=\"htx-overall-assessment\">\n",
        "\t            <Header value=\"Overall Assessment:\"/>\n",
        "\t            <Choices name=\"overall_assessment\" toName=\"chat\" choice=\"single-radio\" showInLine=\"true\" required=\"true\">\n",
        "\t                <Choice value=\"Contains Errors\" hint=\"The conversation contains errors.\"/>\n",
        "\t                <Choice value=\"No Errors\" hint=\"The conversation contains no errors.\"/>\n",
        "\t            </Choices>\n",
        "\t            <Header value=\"Leave notes:\"/>\n",
        "\t\t\t\t<TextArea name=\"overall_assessment_note\" toName=\"chat\" rows=\"10\" maxSubmissions=\"1\" showSubmitButton=\"false\" />\n",
        "\t\t\t</View>\n",
        "\t\t</View>\n",
        "\t\t<View className=\"htx-chat-sidepanel\">\n",
        "\t\t\t<View style=\"position:sticky;top:14px\">\n",
        "\t\t\t\t<View visibleWhen=\"region-selected\" whenRole=\"user\">\n",
        "                    <Header value=\"Select errors:\"/>\n",
        "                    <Choices name=\"user_errors\" toName=\"chat\" choice=\"multiple\">\n",
        "                        <Choice value=\"Request Error\" hint=\"Agent request is not correct or contains errors.\"/>\n",
        "                        <Choice value=\"Other\" hint=\"Other errors.\"/>\n",
        "                    </Choices>\n",
        "                    <Header value=\"Leave notes:\"/>\n",
        "\t\t\t\t\t<TextArea perRegion=\"true\" name=\"user_errors_note\" toName=\"chat\" rows=\"10\" />\n",
        "                </View>\n",
        "\t\t\t\t<View visibleWhen=\"region-selected\" whenRole=\"assistant\">\n",
        "                  \t<Header value=\"Select errors:\"/>\n",
        "                    <Choices name=\"llm_errors\" toName=\"chat\" choice=\"multiple\">\n",
        "                        <Choice value=\"Instruction Adherence\" hint=\"The agent did not follow the instructions provided in the user prompt.\"/>\n",
        "                        <Choice value=\"Format\" hint=\"The message format is not correct or contains errors.\"/>\n",
        "                        <Choice value=\"Hallucination\" hint=\"The response contains information that is not supported by the provided context.\"/>\n",
        "                        <Choice value=\"Bias\" hint=\"The response contains bias or discrimination.\"/>\n",
        "                        <Choice value=\"Toxicity\" hint=\"The response contains toxic or offensive content.\"/>\n",
        "                        <Choice value=\"PII\" hint=\"The response contains Personally Identifiable Information (PII) or any other sensitive information.\"/>\n",
        "                        <Choice value=\"Other\" hint=\"Other errors.\"/>\n",
        "                    </Choices>\n",
        "                    <Header value=\"Leave notes:\"/>\n",
        "\t\t\t\t\t<TextArea perRegion=\"true\" name=\"llm_errors_note\" toName=\"chat\" rows=\"10\" />\n",
        "\t\t\t\t</View>\n",
        "                <View visibleWhen=\"region-selected\" whenRole=\"tool\">\n",
        "                    <Header value=\"Select errors:\"/>\n",
        "                    <Choices name=\"tool_errors\" toName=\"chat\" choice=\"multiple\">\n",
        "                        <Choice value=\"Tool Error\" hint=\"Tool response is not correct or contains errors.\"/>\n",
        "                        <Choice value=\"Other\" hint=\"Other errors.\"/>\n",
        "                    </Choices>\n",
        "                    <Header value=\"Leave notes:\"/>\n",
        "\t\t\t\t\t<TextArea perRegion=\"true\" name=\"tool_errors_note\" toName=\"chat\" rows=\"10\" />\n",
        "                </View>\n",
        "\t\t\t</View>\n",
        "\t\t</View>\n",
        "\t</View>\n",
        "</View>\"\"\"\n",
        "\n",
        "print(\"‚úì Label configuration defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba3a6ded",
      "metadata": {
        "id": "ba3a6ded"
      },
      "source": [
        "### 7. Push Tasks to Label Studio\n",
        "\n",
        "Now we'll create or update the Label Studio project and import all our traces.\n",
        "The import process:\n",
        "\n",
        "1. **Check for existing project**: If a project with the same name exists, we'll use it\n",
        "2. **Create project if needed**: Set up a new project with our chat configuration\n",
        "3. **Import tasks**: Upload all traces with predictions as pre-annotations\n",
        "4. **Generate labeling link**: Provide direct access to the annotation interface\n",
        "\n",
        "### Understanding Label Studio Import\n",
        "\n",
        "When we import tasks with `predictions`, Label Studio creates pre-annotations\n",
        "that appear in the labeling interface. This is powerful because:\n",
        "\n",
        "- **Speeds up annotation**: Annotators see the full conversation immediately\n",
        "- **Maintains context**: All agent steps are visible and evaluable\n",
        "- **Enables comparison**: Multiple annotators can independently assess the same trace\n",
        "- **Quality control**: Supports consensus-based evaluation workflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f0485d",
      "metadata": {
        "id": "d4f0485d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec7bf2d-54c3-4ed6-cc94-7a0c09eee286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing project: pr-another-pamphlet-4\n",
            "\n",
            "‚úì Successfully imported 24 tasks to Label Studio\n",
            "Go to: https://app.humansignal.com/projects/193998/data\n"
          ]
        }
      ],
      "source": [
        "from label_studio_sdk.client import LabelStudio\n",
        "\n",
        "# Initialize Label Studio client\n",
        "ls = LabelStudio(\n",
        "    base_url=os.getenv(\"LABEL_STUDIO_URL\"),\n",
        "    api_key=os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
        ")\n",
        "\n",
        "# Get or create project\n",
        "project_name = os.getenv(\"LANGSMITH_PROJECT\")\n",
        "projects = list(ls.projects.list(title=project_name))\n",
        "\n",
        "if len(projects) == 0:\n",
        "    print(f\"Creating new project: {project_name}\")\n",
        "    project_id = ls.projects.create(\n",
        "        title=project_name,\n",
        "        label_config=label_config\n",
        "    ).id\n",
        "else:\n",
        "    print(f\"Using existing project: {project_name}\")\n",
        "    project_id = projects[0].id\n",
        "\n",
        "# Ensure project has correct label config\n",
        "project = ls.projects.get(int(project_id))\n",
        "if project.label_config.strip() == '<View></View>' or not project.label_config.strip():\n",
        "    print(\"Updating project with chat configuration...\")\n",
        "    ls.projects.update(project_id, label_config=label_config)\n",
        "\n",
        "# Import tasks\n",
        "resp = ls.projects.import_tasks(project_id, request=tasks)\n",
        "print(f\"\\n‚úì Successfully imported {len(tasks)} tasks to Label Studio\")\n",
        "print(f'Go to: {os.getenv(\"LABEL_STUDIO_URL\").rstrip(\"/\")}/projects/{project_id}/data')\n",
        "\n",
        "# Save tasks to file\n",
        "with open('tasks.json', mode='w') as f:\n",
        "    json.dump(tasks, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d209e6e",
      "metadata": {
        "id": "8d209e6e"
      },
      "source": [
        "### 8. Access the Labeling Interface\n",
        "\n",
        "#### The Evaluation Workflow\n",
        "\n",
        "Your traces are now ready for evaluation! The labeling interface provides:\n",
        "\n",
        "##### For Each Message:\n",
        "- Click any message to annotate errors specific to that step\n",
        "- Select from predefined error categories (hallucination, format, bias, etc.)\n",
        "- Add detailed notes explaining the issue\n",
        "\n",
        "##### For Each Conversation:\n",
        "- Overall assessment (Contains Errors / No Errors)\n",
        "- General notes about the entire agent execution\n",
        "\n",
        "##### Multi-User Collaboration:\n",
        "- Multiple SMEs can annotate the same traces\n",
        "- Compare annotations to measure inter-annotator agreement\n",
        "- Use consensus to establish ground truth labels\n",
        "\n",
        "#### Next Steps with Collected Data:\n",
        "\n",
        "1. **Generate Reports**: [Analyze failure modes across all traces](https://docs.humansignal.com/guide/quality)\n",
        "2. **Design Prompts**: [Use annotated examples to improve agent instructions](https://docs.humansignal.com/guide/prompts_overview)\n",
        "3. **LLM-as-a-Judge**: [Create evaluation prompts based on SME feedback](https://docs.humansignal.com/guide/prompts_predictions)\n",
        "4. **Quality Metrics**: [Track error rates over time as you iterate](https://docs.humansignal.com/guide/dashboards)\n",
        "5. **Dataset Creation**: [Build high-quality datasets for fine-tuning](https://docs.humansignal.com/guide/ml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61de8e22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61de8e22",
        "outputId": "40220813-ea7b-4a9b-fec1-48a095dd1d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üéâ SETUP COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "üìä Project: pr-another-pamphlet-4\n",
            "üìù Tasks imported: None\n",
            "\n",
            "üîó Start annotating:\n",
            "   https://app.humansignal.com/projects/193998/labeling\n",
            "\n",
            "üí° Share this link with your Subject Matter Experts to begin evaluation\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate the labeling URL\n",
        "labeling_url = f\"{os.getenv('LABEL_STUDIO_URL')}/projects/{project_id}/labeling\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ SETUP COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìä Project: {project_name}\")\n",
        "print(f\"üìù Tasks imported: {resp.task_count}\")\n",
        "print(f\"\\nüîó Start annotating:\")\n",
        "print(f\"   {labeling_url}\")\n",
        "print(\"\\nüí° Share this link with your Subject Matter Experts to begin evaluation\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fde3d99e",
      "metadata": {
        "id": "fde3d99e"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This tutorial demonstrated the complete workflow from agent execution to expert evaluation:\n",
        "\n",
        "1. ‚úì Set up environment with Langsmith and Label Studio\n",
        "2. ‚úì Ran a ReAct agent with Arxiv search capabilities\n",
        "3. ‚úì Collected traces automatically via Langsmith\n",
        "4. ‚úì Transformed traces into Label Studio's annotation format\n",
        "5. ‚úì Configured a specialized chat UI for failure mode analysis\n",
        "6. ‚úì Imported tasks with pre-annotations\n",
        "7. ‚úì Generated access link for SME annotation\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "While Langsmith excels at technical debugging during development, Label Studio\n",
        "provides the collaborative, expert-driven evaluation framework necessary for\n",
        "production AI systems. The combination creates a powerful workflow:\n",
        "\n",
        "- **Development**: Engineers use Langsmith for debugging\n",
        "- **Evaluation**: SMEs use Label Studio for quality assessment\n",
        "- **Iteration**: Insights from both feed back into agent improvements\n",
        "\n",
        "Happy evaluating! üöÄ\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "- [Langsmith](https://www.langchain.com/langsmith)\n",
        "- [Label Studio](https://docs.humansignal.com/guide/)\n",
        "- [Evals FAQ](https://hamel.dev/blog/posts/evals-faq/)\n",
        "- Video tutorials about AI evals workflows:\n",
        "  - [AI Evaluation Best Practices](https://www.youtube.com/watch?v=52IGjsxdqfI)\n",
        "  - [Building Evaluation Pipelines](https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2s)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}